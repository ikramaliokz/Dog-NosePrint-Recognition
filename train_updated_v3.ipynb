{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 10 images for dog ID 5999\n",
      "Copied 8 images for dog ID 1792\n",
      "Copied 8 images for dog ID 1794\n",
      "Copied 8 images for dog ID 1795\n",
      "Copied 8 images for dog ID 1796\n",
      "Copied 8 images for dog ID 1797\n",
      "Copied 8 images for dog ID 1798\n",
      "Copied 8 images for dog ID 1799\n",
      "Copied 8 images for dog ID 1800\n",
      "Copied 8 images for dog ID 1801\n",
      "Copied 8 images for dog ID 1802\n",
      "Copied 8 images for dog ID 1803\n",
      "Copied 8 images for dog ID 1804\n",
      "Copied 8 images for dog ID 1805\n",
      "Copied 8 images for dog ID 1806\n",
      "Copied 8 images for dog ID 1807\n",
      "Copied 8 images for dog ID 1808\n",
      "Copied 8 images for dog ID 1809\n",
      "Copied 8 images for dog ID 1810\n",
      "Copied 8 images for dog ID 1811\n",
      "Copied 8 images for dog ID 1813\n",
      "Copied 8 images for dog ID 1793\n",
      "Copied 8 images for dog ID 1812\n",
      "Copied 8 images for dog ID 1791\n",
      "Copied 8 images for dog ID 1780\n",
      "Copied 8 images for dog ID 1790\n",
      "Copied 8 images for dog ID 1772\n",
      "Copied 8 images for dog ID 1773\n",
      "Copied 8 images for dog ID 1774\n",
      "Copied 8 images for dog ID 1775\n",
      "Copied 8 images for dog ID 1776\n",
      "Copied 8 images for dog ID 1777\n",
      "Copied 8 images for dog ID 1778\n",
      "Copied 8 images for dog ID 1779\n",
      "Copied 8 images for dog ID 1771\n",
      "Copied 8 images for dog ID 1781\n",
      "Copied 8 images for dog ID 1786\n",
      "Copied 8 images for dog ID 1782\n",
      "Copied 8 images for dog ID 1788\n",
      "Copied 8 images for dog ID 1787\n",
      "Copied 8 images for dog ID 1789\n",
      "Copied 8 images for dog ID 1785\n",
      "Copied 8 images for dog ID 1784\n",
      "Copied 8 images for dog ID 1783\n",
      "Copied 7 images for dog ID 5085\n",
      "Copied 7 images for dog ID 5093\n",
      "Copied 7 images for dog ID 5087\n",
      "Copied 7 images for dog ID 5088\n",
      "Copied 7 images for dog ID 5089\n",
      "Copied 7 images for dog ID 5090\n",
      "Copied 7 images for dog ID 5091\n",
      "Copied 7 images for dog ID 5092\n",
      "Copied 7 images for dog ID 5086\n",
      "Copied 7 images for dog ID 5097\n",
      "Copied 7 images for dog ID 5094\n",
      "Copied 7 images for dog ID 5095\n",
      "Copied 7 images for dog ID 5096\n",
      "Copied 7 images for dog ID 5098\n",
      "Copied 7 images for dog ID 5099\n",
      "Copied 7 images for dog ID 5100\n",
      "Copied 7 images for dog ID 5101\n",
      "Copied 7 images for dog ID 5102\n",
      "Copied 7 images for dog ID 5083\n",
      "Copied 7 images for dog ID 5103\n",
      "Copied 7 images for dog ID 5084\n",
      "Copied 7 images for dog ID 5073\n",
      "Copied 7 images for dog ID 5082\n",
      "Copied 7 images for dog ID 5070\n",
      "Copied 7 images for dog ID 5105\n",
      "Copied 7 images for dog ID 5062\n",
      "Copied 7 images for dog ID 5063\n",
      "Copied 7 images for dog ID 5064\n",
      "Copied 7 images for dog ID 5065\n",
      "Copied 7 images for dog ID 5066\n",
      "Copied 7 images for dog ID 5067\n",
      "Copied 7 images for dog ID 5069\n",
      "Copied 7 images for dog ID 5071\n",
      "Copied 7 images for dog ID 5081\n",
      "Copied 7 images for dog ID 5072\n",
      "Copied 7 images for dog ID 5074\n",
      "Copied 7 images for dog ID 5075\n",
      "Copied 7 images for dog ID 5076\n",
      "Copied 7 images for dog ID 5077\n",
      "Copied 7 images for dog ID 5078\n",
      "Copied 7 images for dog ID 5079\n",
      "Copied 7 images for dog ID 5080\n",
      "Copied 7 images for dog ID 5104\n",
      "Copied 7 images for dog ID 5068\n",
      "Copied 7 images for dog ID 5106\n",
      "Copied 7 images for dog ID 5149\n",
      "Copied 7 images for dog ID 5169\n",
      "Copied 7 images for dog ID 5168\n",
      "Copied 7 images for dog ID 5167\n",
      "Copied 7 images for dog ID 5166\n",
      "Copied 7 images for dog ID 5165\n",
      "Copied 7 images for dog ID 5164\n",
      "Copied 7 images for dog ID 5163\n",
      "Copied 7 images for dog ID 5162\n",
      "Copied 7 images for dog ID 5161\n",
      "Copied 7 images for dog ID 5160\n",
      "Copied 7 images for dog ID 5159\n",
      "Copied 7 images for dog ID 5158\n",
      "Copied 7 images for dog ID 5157\n",
      "Copied 7 images for dog ID 5156\n",
      "Copied 7 images for dog ID 5155\n",
      "Copied 7 images for dog ID 5154\n",
      "Copied 7 images for dog ID 5153\n",
      "Copied 7 images for dog ID 5152\n",
      "Copied 7 images for dog ID 5151\n",
      "Copied 7 images for dog ID 5170\n",
      "Copied 7 images for dog ID 5171\n",
      "Copied 7 images for dog ID 5172\n",
      "Copied 7 images for dog ID 5183\n",
      "Copied 7 images for dog ID 5191\n",
      "Copied 7 images for dog ID 5190\n",
      "Copied 7 images for dog ID 5189\n",
      "Copied 7 images for dog ID 5188\n",
      "Copied 7 images for dog ID 5187\n",
      "Copied 7 images for dog ID 5107\n",
      "Copied 7 images for dog ID 5185\n",
      "Copied 7 images for dog ID 5184\n",
      "Copied 7 images for dog ID 5182\n",
      "Copied 7 images for dog ID 5173\n",
      "Copied 7 images for dog ID 5181\n",
      "Copied 7 images for dog ID 5180\n",
      "Copied 7 images for dog ID 5179\n",
      "Copied 7 images for dog ID 5178\n",
      "Copied 7 images for dog ID 5177\n",
      "Copied 7 images for dog ID 5176\n",
      "Copied 7 images for dog ID 5175\n",
      "Copied 7 images for dog ID 5174\n",
      "Copied 7 images for dog ID 5150\n",
      "Copied 7 images for dog ID 5186\n",
      "Copied 7 images for dog ID 5148\n",
      "Copied 7 images for dog ID 5117\n",
      "Copied 7 images for dog ID 5125\n",
      "Copied 7 images for dog ID 5124\n",
      "Copied 7 images for dog ID 5123\n",
      "Copied 7 images for dog ID 5122\n",
      "Copied 7 images for dog ID 5121\n",
      "Copied 7 images for dog ID 5120\n",
      "Copied 7 images for dog ID 5119\n",
      "Copied 7 images for dog ID 5118\n",
      "Copied 7 images for dog ID 5116\n",
      "Copied 7 images for dog ID 5127\n",
      "Copied 7 images for dog ID 5115\n",
      "Copied 7 images for dog ID 5114\n",
      "Copied 7 images for dog ID 5113\n",
      "Copied 7 images for dog ID 5112\n",
      "Copied 7 images for dog ID 5111\n",
      "Copied 7 images for dog ID 5109\n",
      "Copied 7 images for dog ID 5108\n",
      "Copied 7 images for dog ID 5147\n",
      "Copied 7 images for dog ID 5126\n",
      "Copied 7 images for dog ID 5110\n",
      "Copied 7 images for dog ID 5128\n",
      "Copied 7 images for dog ID 5138\n",
      "Copied 7 images for dog ID 5129\n",
      "Copied 7 images for dog ID 5145\n",
      "Copied 7 images for dog ID 5146\n",
      "Copied 7 images for dog ID 5143\n",
      "Copied 7 images for dog ID 5142\n",
      "Copied 7 images for dog ID 5141\n",
      "Copied 7 images for dog ID 5140\n",
      "Copied 7 images for dog ID 5139\n",
      "Copied 7 images for dog ID 5144\n",
      "Copied 7 images for dog ID 5137\n",
      "Copied 7 images for dog ID 5135\n",
      "Copied 7 images for dog ID 5134\n",
      "Copied 7 images for dog ID 5133\n",
      "Copied 7 images for dog ID 5132\n",
      "Copied 7 images for dog ID 5131\n",
      "Copied 7 images for dog ID 5136\n",
      "Copied 7 images for dog ID 5130\n",
      "Copied 6 images for dog ID 5721\n",
      "Copied 6 images for dog ID 5728\n",
      "Copied 6 images for dog ID 5727\n",
      "Copied 6 images for dog ID 5726\n",
      "Copied 6 images for dog ID 5725\n",
      "Copied 6 images for dog ID 5724\n",
      "Copied 6 images for dog ID 5723\n",
      "Copied 6 images for dog ID 5722\n",
      "Copied 6 images for dog ID 5718\n",
      "Copied 6 images for dog ID 5720\n",
      "Copied 6 images for dog ID 5719\n",
      "Copied 6 images for dog ID 5717\n",
      "Copied 6 images for dog ID 5716\n",
      "Copied 6 images for dog ID 5715\n",
      "Copied 6 images for dog ID 5714\n",
      "Copied 6 images for dog ID 5713\n",
      "Copied 6 images for dog ID 5712\n",
      "Copied 6 images for dog ID 5729\n",
      "Copied 6 images for dog ID 5738\n",
      "Copied 6 images for dog ID 5730\n",
      "Copied 6 images for dog ID 5741\n",
      "Copied 6 images for dog ID 5747\n",
      "Copied 6 images for dog ID 5710\n",
      "Copied 6 images for dog ID 5746\n",
      "Copied 6 images for dog ID 5745\n",
      "Copied 6 images for dog ID 5744\n",
      "Copied 6 images for dog ID 5743\n",
      "Copied 6 images for dog ID 5742\n",
      "Copied 6 images for dog ID 5740\n",
      "Copied 6 images for dog ID 5731\n",
      "Copied 6 images for dog ID 5739\n",
      "Copied 6 images for dog ID 5737\n",
      "Copied 6 images for dog ID 5736\n",
      "Copied 6 images for dog ID 5735\n",
      "Copied 6 images for dog ID 5734\n",
      "Copied 6 images for dog ID 5733\n",
      "Copied 6 images for dog ID 5732\n",
      "Copied 6 images for dog ID 5711\n",
      "Copied 6 images for dog ID 5690\n",
      "Copied 6 images for dog ID 5709\n",
      "Copied 6 images for dog ID 5680\n",
      "Copied 6 images for dog ID 5687\n",
      "Copied 6 images for dog ID 5686\n",
      "Copied 6 images for dog ID 5685\n",
      "Copied 6 images for dog ID 5684\n",
      "Copied 6 images for dog ID 5683\n",
      "Copied 6 images for dog ID 5682\n",
      "Copied 6 images for dog ID 5681\n",
      "Copied 6 images for dog ID 5679\n",
      "Copied 6 images for dog ID 5708\n",
      "Copied 6 images for dog ID 5678\n",
      "Copied 6 images for dog ID 5677\n",
      "Copied 6 images for dog ID 5676\n",
      "Copied 6 images for dog ID 5675\n",
      "Copied 6 images for dog ID 5674\n",
      "Copied 6 images for dog ID 5672\n",
      "Copied 6 images for dog ID 5749\n",
      "Copied 6 images for dog ID 5688\n",
      "Copied 6 images for dog ID 5689\n",
      "Copied 6 images for dog ID 5691\n",
      "Copied 6 images for dog ID 5692\n",
      "Copied 6 images for dog ID 5707\n",
      "Copied 6 images for dog ID 5706\n",
      "Copied 6 images for dog ID 5705\n",
      "Copied 6 images for dog ID 5704\n",
      "Copied 6 images for dog ID 5703\n",
      "Copied 6 images for dog ID 5702\n",
      "Copied 6 images for dog ID 5701\n",
      "Copied 6 images for dog ID 5700\n",
      "Copied 6 images for dog ID 5699\n",
      "Copied 6 images for dog ID 5698\n",
      "Copied 6 images for dog ID 5697\n",
      "Copied 6 images for dog ID 5696\n",
      "Copied 6 images for dog ID 5695\n",
      "Copied 6 images for dog ID 5694\n",
      "Copied 6 images for dog ID 5693\n",
      "Copied 6 images for dog ID 5748\n",
      "Copied 6 images for dog ID 5673\n",
      "Copied 6 images for dog ID 5750\n",
      "Copied 6 images for dog ID 5921\n",
      "Copied 6 images for dog ID 5908\n",
      "Copied 6 images for dog ID 5909\n",
      "Copied 6 images for dog ID 5910\n",
      "Copied 6 images for dog ID 5911\n",
      "Copied 6 images for dog ID 5912\n",
      "Copied 6 images for dog ID 5913\n",
      "Copied 6 images for dog ID 5914\n",
      "Copied 6 images for dog ID 5915\n",
      "Copied 6 images for dog ID 5916\n",
      "Copied 6 images for dog ID 5917\n",
      "Copied 6 images for dog ID 5918\n",
      "Copied 6 images for dog ID 5919\n",
      "Copied 6 images for dog ID 5920\n",
      "Copied 6 images for dog ID 5922\n",
      "Copied 6 images for dog ID 5937\n",
      "Copied 6 images for dog ID 5923\n",
      "Copied 6 images for dog ID 5924\n",
      "Copied 6 images for dog ID 5925\n",
      "Copied 6 images for dog ID 5926\n",
      "Copied 6 images for dog ID 5927\n",
      "Copied 6 images for dog ID 5928\n",
      "Copied 6 images for dog ID 5929\n",
      "Copied 6 images for dog ID 5930\n",
      "Copied 6 images for dog ID 5931\n",
      "Copied 6 images for dog ID 5932\n",
      "Copied 6 images for dog ID 5933\n",
      "Copied 6 images for dog ID 5934\n",
      "Copied 6 images for dog ID 5935\n",
      "Copied 6 images for dog ID 5907\n",
      "Copied 6 images for dog ID 5906\n",
      "Copied 6 images for dog ID 5905\n",
      "Copied 6 images for dog ID 5904\n",
      "Copied 6 images for dog ID 5877\n",
      "Copied 6 images for dog ID 5878\n",
      "Copied 6 images for dog ID 5879\n",
      "Copied 6 images for dog ID 5880\n",
      "Copied 6 images for dog ID 5881\n",
      "Copied 6 images for dog ID 5882\n",
      "Copied 6 images for dog ID 5883\n",
      "Copied 6 images for dog ID 5884\n",
      "Copied 6 images for dog ID 5885\n",
      "Copied 6 images for dog ID 5886\n",
      "Copied 6 images for dog ID 5887\n",
      "Copied 6 images for dog ID 5888\n",
      "Copied 6 images for dog ID 5889\n",
      "Copied 6 images for dog ID 5890\n",
      "Copied 6 images for dog ID 5891\n",
      "Copied 6 images for dog ID 5892\n",
      "Copied 6 images for dog ID 5893\n",
      "Copied 6 images for dog ID 5894\n",
      "Copied 6 images for dog ID 5895\n",
      "Copied 6 images for dog ID 5896\n",
      "Copied 6 images for dog ID 5897\n",
      "Copied 6 images for dog ID 5898\n",
      "Copied 6 images for dog ID 5899\n",
      "Copied 6 images for dog ID 5900\n",
      "Copied 6 images for dog ID 5901\n",
      "Copied 6 images for dog ID 5902\n",
      "Copied 6 images for dog ID 5903\n",
      "Copied 6 images for dog ID 5936\n",
      "Copied 6 images for dog ID 5938\n",
      "Copied 6 images for dog ID 5875\n",
      "Copied 6 images for dog ID 5984\n",
      "Copied 6 images for dog ID 5971\n",
      "Copied 6 images for dog ID 5972\n",
      "Copied 6 images for dog ID 5973\n",
      "Copied 6 images for dog ID 5974\n",
      "Copied 6 images for dog ID 5975\n",
      "Copied 6 images for dog ID 5976\n",
      "Copied 6 images for dog ID 5977\n",
      "Copied 6 images for dog ID 5978\n",
      "Copied 6 images for dog ID 5979\n",
      "Copied 6 images for dog ID 5980\n",
      "Copied 6 images for dog ID 5981\n",
      "Copied 6 images for dog ID 5982\n",
      "Copied 6 images for dog ID 5983\n",
      "Copied 6 images for dog ID 5985\n",
      "Copied 6 images for dog ID 5939\n",
      "Copied 6 images for dog ID 5986\n",
      "Copied 6 images for dog ID 5987\n",
      "Copied 6 images for dog ID 5988\n",
      "Copied 6 images for dog ID 5989\n",
      "Copied 6 images for dog ID 5990\n",
      "Copied 6 images for dog ID 5991\n",
      "Copied 6 images for dog ID 5992\n",
      "Copied 6 images for dog ID 5993\n",
      "Copied 6 images for dog ID 5994\n",
      "Copied 6 images for dog ID 5995\n",
      "Copied 6 images for dog ID 5996\n",
      "Copied 6 images for dog ID 5997\n",
      "Copied 6 images for dog ID 5998\n",
      "Copied 6 images for dog ID 5970\n",
      "Copied 6 images for dog ID 5969\n",
      "Copied 6 images for dog ID 5968\n",
      "Copied 6 images for dog ID 5967\n",
      "Copied 6 images for dog ID 5940\n",
      "Copied 6 images for dog ID 5941\n",
      "Copied 6 images for dog ID 5942\n",
      "Copied 6 images for dog ID 5943\n",
      "Copied 6 images for dog ID 5944\n",
      "Copied 6 images for dog ID 5945\n",
      "Copied 6 images for dog ID 5946\n",
      "Copied 6 images for dog ID 5947\n",
      "Copied 6 images for dog ID 5948\n",
      "Copied 6 images for dog ID 5949\n",
      "Copied 6 images for dog ID 5751\n",
      "Copied 6 images for dog ID 5951\n",
      "Copied 6 images for dog ID 5952\n",
      "Copied 6 images for dog ID 5953\n",
      "Copied 6 images for dog ID 5954\n",
      "Copied 6 images for dog ID 5955\n",
      "Copied 6 images for dog ID 5956\n",
      "Copied 6 images for dog ID 5957\n",
      "Copied 6 images for dog ID 5958\n",
      "Copied 6 images for dog ID 5959\n",
      "Copied 6 images for dog ID 5960\n",
      "Copied 6 images for dog ID 5961\n",
      "Copied 6 images for dog ID 5962\n",
      "Copied 6 images for dog ID 5963\n",
      "Copied 6 images for dog ID 5964\n",
      "Copied 6 images for dog ID 5965\n",
      "Copied 6 images for dog ID 5966\n",
      "Copied 6 images for dog ID 5876\n",
      "Copied 6 images for dog ID 5950\n",
      "Copied 6 images for dog ID 5874\n",
      "Copied 6 images for dog ID 5796\n",
      "Copied 6 images for dog ID 5783\n",
      "Copied 6 images for dog ID 5784\n",
      "Copied 6 images for dog ID 5785\n",
      "Copied 6 images for dog ID 5786\n",
      "Copied 6 images for dog ID 5787\n",
      "Copied 6 images for dog ID 5788\n",
      "Copied 6 images for dog ID 5789\n",
      "Copied 6 images for dog ID 5790\n",
      "Copied 6 images for dog ID 5791\n",
      "Copied 6 images for dog ID 5792\n",
      "Copied 6 images for dog ID 5793\n",
      "Copied 6 images for dog ID 5794\n",
      "Copied 6 images for dog ID 5795\n",
      "Copied 6 images for dog ID 5797\n",
      "Copied 6 images for dog ID 5812\n",
      "Copied 6 images for dog ID 5798\n",
      "Copied 6 images for dog ID 5799\n",
      "Copied 6 images for dog ID 5800\n",
      "Copied 6 images for dog ID 5801\n",
      "Copied 6 images for dog ID 5802\n",
      "Copied 6 images for dog ID 5803\n",
      "Copied 6 images for dog ID 5804\n",
      "Copied 6 images for dog ID 5805\n",
      "Copied 6 images for dog ID 5806\n",
      "Copied 6 images for dog ID 5807\n",
      "Copied 6 images for dog ID 5808\n",
      "Copied 6 images for dog ID 5809\n",
      "Copied 6 images for dog ID 5810\n",
      "Copied 6 images for dog ID 5782\n",
      "Copied 6 images for dog ID 5781\n",
      "Copied 6 images for dog ID 5780\n",
      "Copied 6 images for dog ID 5779\n",
      "Copied 6 images for dog ID 5873\n",
      "Copied 6 images for dog ID 5752\n",
      "Copied 6 images for dog ID 5753\n",
      "Copied 6 images for dog ID 5754\n",
      "Copied 6 images for dog ID 5755\n",
      "Copied 6 images for dog ID 5756\n",
      "Copied 6 images for dog ID 5757\n",
      "Copied 6 images for dog ID 5758\n",
      "Copied 6 images for dog ID 5759\n",
      "Copied 6 images for dog ID 5760\n",
      "Copied 6 images for dog ID 5761\n",
      "Copied 6 images for dog ID 5762\n",
      "Copied 6 images for dog ID 5763\n",
      "Copied 6 images for dog ID 5764\n",
      "Copied 6 images for dog ID 5765\n",
      "Copied 6 images for dog ID 5766\n",
      "Copied 6 images for dog ID 5767\n",
      "Copied 6 images for dog ID 5768\n",
      "Copied 6 images for dog ID 5769\n",
      "Copied 6 images for dog ID 5770\n",
      "Copied 6 images for dog ID 5771\n",
      "Copied 6 images for dog ID 5772\n",
      "Copied 6 images for dog ID 5773\n",
      "Copied 6 images for dog ID 5774\n",
      "Copied 6 images for dog ID 5775\n",
      "Copied 6 images for dog ID 5777\n",
      "Copied 6 images for dog ID 5778\n",
      "Copied 6 images for dog ID 5811\n",
      "Copied 6 images for dog ID 5776\n",
      "Copied 6 images for dog ID 5813\n",
      "Copied 6 images for dog ID 5858\n",
      "Copied 6 images for dog ID 5845\n",
      "Copied 6 images for dog ID 5846\n",
      "Copied 6 images for dog ID 5847\n",
      "Copied 6 images for dog ID 5848\n",
      "Copied 6 images for dog ID 5849\n",
      "Copied 6 images for dog ID 5850\n",
      "Copied 6 images for dog ID 5851\n",
      "Copied 6 images for dog ID 5852\n",
      "Copied 6 images for dog ID 5853\n",
      "Copied 6 images for dog ID 5854\n",
      "Copied 6 images for dog ID 5855\n",
      "Copied 6 images for dog ID 5856\n",
      "Copied 6 images for dog ID 5857\n",
      "Copied 6 images for dog ID 5860\n",
      "Copied 6 images for dog ID 5843\n",
      "Copied 6 images for dog ID 5861\n",
      "Copied 6 images for dog ID 5862\n",
      "Copied 6 images for dog ID 5863\n",
      "Copied 6 images for dog ID 5864\n",
      "Copied 6 images for dog ID 5865\n",
      "Copied 6 images for dog ID 5866\n",
      "Copied 6 images for dog ID 5867\n",
      "Copied 6 images for dog ID 5868\n",
      "Copied 6 images for dog ID 5869\n",
      "Copied 6 images for dog ID 5870\n",
      "Copied 6 images for dog ID 5871\n",
      "Copied 6 images for dog ID 5814\n",
      "Copied 6 images for dog ID 5872\n",
      "Copied 6 images for dog ID 5844\n",
      "Copied 6 images for dog ID 5859\n",
      "Copied 6 images for dog ID 5842\n",
      "Copied 6 images for dog ID 5828\n",
      "Copied 6 images for dog ID 5815\n",
      "Copied 6 images for dog ID 5816\n",
      "Copied 6 images for dog ID 5841\n",
      "Copied 6 images for dog ID 5817\n",
      "Copied 6 images for dog ID 5818\n",
      "Copied 6 images for dog ID 5819\n",
      "Copied 6 images for dog ID 5820\n",
      "Copied 6 images for dog ID 5821\n",
      "Copied 6 images for dog ID 5823\n",
      "Copied 6 images for dog ID 5824\n",
      "Copied 6 images for dog ID 5825\n",
      "Copied 6 images for dog ID 5826\n",
      "Copied 6 images for dog ID 5827\n",
      "Copied 6 images for dog ID 5822\n",
      "Copied 6 images for dog ID 5836\n",
      "Copied 6 images for dog ID 5829\n",
      "Copied 6 images for dog ID 5830\n",
      "Copied 6 images for dog ID 5831\n",
      "Copied 6 images for dog ID 5832\n",
      "Copied 6 images for dog ID 5833\n",
      "Copied 6 images for dog ID 5840\n",
      "Copied 6 images for dog ID 5834\n",
      "Copied 6 images for dog ID 5835\n",
      "Copied 6 images for dog ID 5839\n",
      "Copied 6 images for dog ID 5837\n",
      "Copied 6 images for dog ID 5838\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('pet_biometric_challenge_2022/train/train_data.csv')\n",
    "\n",
    "# Count the number of images for each dog ID\n",
    "image_counts = df['dog ID'].value_counts()\n",
    "\n",
    "# Filter IDs with 5 or more images\n",
    "ids_with_enough_images = image_counts[image_counts >= 6].index\n",
    "\n",
    "# Create directories and copy files\n",
    "for dog_id in ids_with_enough_images:\n",
    "    # Create a directory for the dog ID if it doesn't exist\n",
    "    directory_path = f'./dataset_with_morethan_6imgs/{dog_id}'\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    # Get all images for this dog ID\n",
    "    images_to_copy = df[df['dog ID'] == dog_id]['nose print image']\n",
    "    \n",
    "    # Copy each image\n",
    "    for image in images_to_copy:\n",
    "        src_path = f'pet_biometric_challenge_2022/train/images/{image}'  # Adjust this path\n",
    "        dst_path = f'{directory_path}/{image}'\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print(f'Copied {len(images_to_copy)} images for dog ID {dog_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 6 images for dog ID 5721\n",
      "Copied 6 images for dog ID 5728\n",
      "Copied 6 images for dog ID 5727\n",
      "Copied 6 images for dog ID 5726\n",
      "Copied 6 images for dog ID 5725\n",
      "Copied 6 images for dog ID 5724\n",
      "Copied 6 images for dog ID 5723\n",
      "Copied 6 images for dog ID 5722\n",
      "Copied 6 images for dog ID 5718\n",
      "Copied 6 images for dog ID 5720\n",
      "Copied 6 images for dog ID 5719\n",
      "Copied 6 images for dog ID 5717\n",
      "Copied 6 images for dog ID 5716\n",
      "Copied 6 images for dog ID 5715\n",
      "Copied 6 images for dog ID 5714\n",
      "Copied 6 images for dog ID 5713\n",
      "Copied 6 images for dog ID 5712\n",
      "Copied 6 images for dog ID 5729\n",
      "Copied 6 images for dog ID 5738\n",
      "Copied 6 images for dog ID 5730\n",
      "Copied 6 images for dog ID 5741\n",
      "Copied 6 images for dog ID 5747\n",
      "Copied 6 images for dog ID 5710\n",
      "Copied 6 images for dog ID 5746\n",
      "Copied 6 images for dog ID 5745\n",
      "Copied 6 images for dog ID 5744\n",
      "Copied 6 images for dog ID 5743\n",
      "Copied 6 images for dog ID 5742\n",
      "Copied 6 images for dog ID 5740\n",
      "Copied 6 images for dog ID 5731\n",
      "Copied 6 images for dog ID 5739\n",
      "Copied 6 images for dog ID 5737\n",
      "Copied 6 images for dog ID 5736\n",
      "Copied 6 images for dog ID 5735\n",
      "Copied 6 images for dog ID 5734\n",
      "Copied 6 images for dog ID 5733\n",
      "Copied 6 images for dog ID 5732\n",
      "Copied 6 images for dog ID 5711\n",
      "Copied 6 images for dog ID 5690\n",
      "Copied 6 images for dog ID 5709\n",
      "Copied 6 images for dog ID 5680\n",
      "Copied 6 images for dog ID 5687\n",
      "Copied 6 images for dog ID 5686\n",
      "Copied 6 images for dog ID 5685\n",
      "Copied 6 images for dog ID 5684\n",
      "Copied 6 images for dog ID 5683\n",
      "Copied 6 images for dog ID 5682\n",
      "Copied 6 images for dog ID 5681\n",
      "Copied 6 images for dog ID 5679\n",
      "Copied 6 images for dog ID 5708\n",
      "Copied 6 images for dog ID 5678\n",
      "Copied 6 images for dog ID 5677\n",
      "Copied 6 images for dog ID 5676\n",
      "Copied 6 images for dog ID 5675\n",
      "Copied 6 images for dog ID 5674\n",
      "Copied 6 images for dog ID 5672\n",
      "Copied 6 images for dog ID 5749\n",
      "Copied 6 images for dog ID 5688\n",
      "Copied 6 images for dog ID 5689\n",
      "Copied 6 images for dog ID 5691\n",
      "Copied 6 images for dog ID 5692\n",
      "Copied 6 images for dog ID 5707\n",
      "Copied 6 images for dog ID 5706\n",
      "Copied 6 images for dog ID 5705\n",
      "Copied 6 images for dog ID 5704\n",
      "Copied 6 images for dog ID 5703\n",
      "Copied 6 images for dog ID 5702\n",
      "Copied 6 images for dog ID 5701\n",
      "Copied 6 images for dog ID 5700\n",
      "Copied 6 images for dog ID 5699\n",
      "Copied 6 images for dog ID 5698\n",
      "Copied 6 images for dog ID 5697\n",
      "Copied 6 images for dog ID 5696\n",
      "Copied 6 images for dog ID 5695\n",
      "Copied 6 images for dog ID 5694\n",
      "Copied 6 images for dog ID 5693\n",
      "Copied 6 images for dog ID 5748\n",
      "Copied 6 images for dog ID 5673\n",
      "Copied 6 images for dog ID 5750\n",
      "Copied 6 images for dog ID 5921\n",
      "Copied 6 images for dog ID 5908\n",
      "Copied 6 images for dog ID 5909\n",
      "Copied 6 images for dog ID 5910\n",
      "Copied 6 images for dog ID 5911\n",
      "Copied 6 images for dog ID 5912\n",
      "Copied 6 images for dog ID 5913\n",
      "Copied 6 images for dog ID 5914\n",
      "Copied 6 images for dog ID 5915\n",
      "Copied 6 images for dog ID 5916\n",
      "Copied 6 images for dog ID 5917\n",
      "Copied 6 images for dog ID 5918\n",
      "Copied 6 images for dog ID 5919\n",
      "Copied 6 images for dog ID 5920\n",
      "Copied 6 images for dog ID 5922\n",
      "Copied 6 images for dog ID 5937\n",
      "Copied 6 images for dog ID 5923\n",
      "Copied 6 images for dog ID 5924\n",
      "Copied 6 images for dog ID 5925\n",
      "Copied 6 images for dog ID 5926\n",
      "Copied 6 images for dog ID 5927\n",
      "Copied 6 images for dog ID 5928\n",
      "Copied 6 images for dog ID 5929\n",
      "Copied 6 images for dog ID 5930\n",
      "Copied 6 images for dog ID 5931\n",
      "Copied 6 images for dog ID 5932\n",
      "Copied 6 images for dog ID 5933\n",
      "Copied 6 images for dog ID 5934\n",
      "Copied 6 images for dog ID 5935\n",
      "Copied 6 images for dog ID 5907\n",
      "Copied 6 images for dog ID 5906\n",
      "Copied 6 images for dog ID 5905\n",
      "Copied 6 images for dog ID 5904\n",
      "Copied 6 images for dog ID 5877\n",
      "Copied 6 images for dog ID 5878\n",
      "Copied 6 images for dog ID 5879\n",
      "Copied 6 images for dog ID 5880\n",
      "Copied 6 images for dog ID 5881\n",
      "Copied 6 images for dog ID 5882\n",
      "Copied 6 images for dog ID 5883\n",
      "Copied 6 images for dog ID 5884\n",
      "Copied 6 images for dog ID 5885\n",
      "Copied 6 images for dog ID 5886\n",
      "Copied 6 images for dog ID 5887\n",
      "Copied 6 images for dog ID 5888\n",
      "Copied 6 images for dog ID 5889\n",
      "Copied 6 images for dog ID 5890\n",
      "Copied 6 images for dog ID 5891\n",
      "Copied 6 images for dog ID 5892\n",
      "Copied 6 images for dog ID 5893\n",
      "Copied 6 images for dog ID 5894\n",
      "Copied 6 images for dog ID 5895\n",
      "Copied 6 images for dog ID 5896\n",
      "Copied 6 images for dog ID 5897\n",
      "Copied 6 images for dog ID 5898\n",
      "Copied 6 images for dog ID 5899\n",
      "Copied 6 images for dog ID 5900\n",
      "Copied 6 images for dog ID 5901\n",
      "Copied 6 images for dog ID 5902\n",
      "Copied 6 images for dog ID 5903\n",
      "Copied 6 images for dog ID 5936\n",
      "Copied 6 images for dog ID 5938\n",
      "Copied 6 images for dog ID 5875\n",
      "Copied 6 images for dog ID 5984\n",
      "Copied 6 images for dog ID 5971\n",
      "Copied 6 images for dog ID 5972\n",
      "Copied 6 images for dog ID 5973\n",
      "Copied 6 images for dog ID 5974\n",
      "Copied 6 images for dog ID 5975\n",
      "Copied 6 images for dog ID 5976\n",
      "Copied 6 images for dog ID 5977\n",
      "Copied 6 images for dog ID 5978\n",
      "Copied 6 images for dog ID 5979\n",
      "Copied 6 images for dog ID 5980\n",
      "Copied 6 images for dog ID 5981\n",
      "Copied 6 images for dog ID 5982\n",
      "Copied 6 images for dog ID 5983\n",
      "Copied 6 images for dog ID 5985\n",
      "Copied 6 images for dog ID 5939\n",
      "Copied 6 images for dog ID 5986\n",
      "Copied 6 images for dog ID 5987\n",
      "Copied 6 images for dog ID 5988\n",
      "Copied 6 images for dog ID 5989\n",
      "Copied 6 images for dog ID 5990\n",
      "Copied 6 images for dog ID 5991\n",
      "Copied 6 images for dog ID 5992\n",
      "Copied 6 images for dog ID 5993\n",
      "Copied 6 images for dog ID 5994\n",
      "Copied 6 images for dog ID 5995\n",
      "Copied 6 images for dog ID 5996\n",
      "Copied 6 images for dog ID 5997\n",
      "Copied 6 images for dog ID 5998\n",
      "Copied 6 images for dog ID 5970\n",
      "Copied 6 images for dog ID 5969\n",
      "Copied 6 images for dog ID 5968\n",
      "Copied 6 images for dog ID 5967\n",
      "Copied 6 images for dog ID 5940\n",
      "Copied 6 images for dog ID 5941\n",
      "Copied 6 images for dog ID 5942\n",
      "Copied 6 images for dog ID 5943\n",
      "Copied 6 images for dog ID 5944\n",
      "Copied 6 images for dog ID 5945\n",
      "Copied 6 images for dog ID 5946\n",
      "Copied 6 images for dog ID 5947\n",
      "Copied 6 images for dog ID 5948\n",
      "Copied 6 images for dog ID 5949\n",
      "Copied 6 images for dog ID 5751\n",
      "Copied 6 images for dog ID 5951\n",
      "Copied 6 images for dog ID 5952\n",
      "Copied 6 images for dog ID 5953\n",
      "Copied 6 images for dog ID 5954\n",
      "Copied 6 images for dog ID 5955\n",
      "Copied 6 images for dog ID 5956\n",
      "Copied 6 images for dog ID 5957\n",
      "Copied 6 images for dog ID 5958\n",
      "Copied 6 images for dog ID 5959\n",
      "Copied 6 images for dog ID 5960\n",
      "Copied 6 images for dog ID 5961\n",
      "Copied 6 images for dog ID 5962\n",
      "Copied 6 images for dog ID 5963\n",
      "Copied 6 images for dog ID 5964\n",
      "Copied 6 images for dog ID 5965\n",
      "Copied 6 images for dog ID 5966\n",
      "Copied 6 images for dog ID 5876\n",
      "Copied 6 images for dog ID 5950\n",
      "Copied 6 images for dog ID 5874\n",
      "Copied 6 images for dog ID 5796\n",
      "Copied 6 images for dog ID 5783\n",
      "Copied 6 images for dog ID 5784\n",
      "Copied 6 images for dog ID 5785\n",
      "Copied 6 images for dog ID 5786\n",
      "Copied 6 images for dog ID 5787\n",
      "Copied 6 images for dog ID 5788\n",
      "Copied 6 images for dog ID 5789\n",
      "Copied 6 images for dog ID 5790\n",
      "Copied 6 images for dog ID 5791\n",
      "Copied 6 images for dog ID 5792\n",
      "Copied 6 images for dog ID 5793\n",
      "Copied 6 images for dog ID 5794\n",
      "Copied 6 images for dog ID 5795\n",
      "Copied 6 images for dog ID 5797\n",
      "Copied 6 images for dog ID 5812\n",
      "Copied 6 images for dog ID 5798\n",
      "Copied 6 images for dog ID 5799\n",
      "Copied 6 images for dog ID 5800\n",
      "Copied 6 images for dog ID 5801\n",
      "Copied 6 images for dog ID 5802\n",
      "Copied 6 images for dog ID 5803\n",
      "Copied 6 images for dog ID 5804\n",
      "Copied 6 images for dog ID 5805\n",
      "Copied 6 images for dog ID 5806\n",
      "Copied 6 images for dog ID 5807\n",
      "Copied 6 images for dog ID 5808\n",
      "Copied 6 images for dog ID 5809\n",
      "Copied 6 images for dog ID 5810\n",
      "Copied 6 images for dog ID 5782\n",
      "Copied 6 images for dog ID 5781\n",
      "Copied 6 images for dog ID 5780\n",
      "Copied 6 images for dog ID 5779\n",
      "Copied 6 images for dog ID 5873\n",
      "Copied 6 images for dog ID 5752\n",
      "Copied 6 images for dog ID 5753\n",
      "Copied 6 images for dog ID 5754\n",
      "Copied 6 images for dog ID 5755\n",
      "Copied 6 images for dog ID 5756\n",
      "Copied 6 images for dog ID 5757\n",
      "Copied 6 images for dog ID 5758\n",
      "Copied 6 images for dog ID 5759\n",
      "Copied 6 images for dog ID 5760\n",
      "Copied 6 images for dog ID 5761\n",
      "Copied 6 images for dog ID 5762\n",
      "Copied 6 images for dog ID 5763\n",
      "Copied 6 images for dog ID 5764\n",
      "Copied 6 images for dog ID 5765\n",
      "Copied 6 images for dog ID 5766\n",
      "Copied 6 images for dog ID 5767\n",
      "Copied 6 images for dog ID 5768\n",
      "Copied 6 images for dog ID 5769\n",
      "Copied 6 images for dog ID 5770\n",
      "Copied 6 images for dog ID 5771\n",
      "Copied 6 images for dog ID 5772\n",
      "Copied 6 images for dog ID 5773\n",
      "Copied 6 images for dog ID 5774\n",
      "Copied 6 images for dog ID 5775\n",
      "Copied 6 images for dog ID 5777\n",
      "Copied 6 images for dog ID 5778\n",
      "Copied 6 images for dog ID 5811\n",
      "Copied 6 images for dog ID 5776\n",
      "Copied 6 images for dog ID 5813\n",
      "Copied 6 images for dog ID 5858\n",
      "Copied 6 images for dog ID 5845\n",
      "Copied 6 images for dog ID 5846\n",
      "Copied 6 images for dog ID 5847\n",
      "Copied 6 images for dog ID 5848\n",
      "Copied 6 images for dog ID 5849\n",
      "Copied 6 images for dog ID 5850\n",
      "Copied 6 images for dog ID 5851\n",
      "Copied 6 images for dog ID 5852\n",
      "Copied 6 images for dog ID 5853\n",
      "Copied 6 images for dog ID 5854\n",
      "Copied 6 images for dog ID 5855\n",
      "Copied 6 images for dog ID 5856\n",
      "Copied 6 images for dog ID 5857\n",
      "Copied 6 images for dog ID 5860\n",
      "Copied 6 images for dog ID 5843\n",
      "Copied 6 images for dog ID 5861\n",
      "Copied 6 images for dog ID 5862\n",
      "Copied 6 images for dog ID 5863\n",
      "Copied 6 images for dog ID 5864\n",
      "Copied 6 images for dog ID 5865\n",
      "Copied 6 images for dog ID 5866\n",
      "Copied 6 images for dog ID 5867\n",
      "Copied 6 images for dog ID 5868\n",
      "Copied 6 images for dog ID 5869\n",
      "Copied 6 images for dog ID 5870\n",
      "Copied 6 images for dog ID 5871\n",
      "Copied 6 images for dog ID 5814\n",
      "Copied 6 images for dog ID 5872\n",
      "Copied 6 images for dog ID 5844\n",
      "Copied 6 images for dog ID 5859\n",
      "Copied 6 images for dog ID 5842\n",
      "Copied 6 images for dog ID 5828\n",
      "Copied 6 images for dog ID 5815\n",
      "Copied 6 images for dog ID 5816\n",
      "Copied 6 images for dog ID 5841\n",
      "Copied 6 images for dog ID 5817\n",
      "Copied 6 images for dog ID 5818\n",
      "Copied 6 images for dog ID 5819\n",
      "Copied 6 images for dog ID 5820\n",
      "Copied 6 images for dog ID 5821\n",
      "Copied 6 images for dog ID 5823\n",
      "Copied 6 images for dog ID 5824\n",
      "Copied 6 images for dog ID 5825\n",
      "Copied 6 images for dog ID 5826\n",
      "Copied 6 images for dog ID 5827\n",
      "Copied 6 images for dog ID 5822\n",
      "Copied 6 images for dog ID 5836\n",
      "Copied 6 images for dog ID 5829\n",
      "Copied 6 images for dog ID 5830\n",
      "Copied 6 images for dog ID 5831\n",
      "Copied 6 images for dog ID 5832\n",
      "Copied 6 images for dog ID 5833\n",
      "Copied 6 images for dog ID 5840\n",
      "Copied 6 images for dog ID 5834\n",
      "Copied 6 images for dog ID 5835\n",
      "Copied 6 images for dog ID 5839\n",
      "Copied 6 images for dog ID 5837\n",
      "Copied 6 images for dog ID 5838\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('pet_biometric_challenge_2022/train/train_data.csv')\n",
    "\n",
    "# Count the number of images for each dog ID\n",
    "image_counts = df['dog ID'].value_counts()\n",
    "\n",
    "# Filter IDs with 5 or more images\n",
    "ids_with_enough_images = image_counts[image_counts == 6].index\n",
    "\n",
    "# Create directories and copy files\n",
    "for dog_id in ids_with_enough_images:\n",
    "    # Create a directory for the dog ID if it doesn't exist\n",
    "    directory_path = f'./dataset2/classes_with_6imgs/{dog_id}'\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    # Get all images for this dog ID\n",
    "    images_to_copy = df[df['dog ID'] == dog_id]['nose print image']\n",
    "    \n",
    "    # Copy each image\n",
    "    for image in images_to_copy:\n",
    "        src_path = f'pet_biometric_challenge_2022/train/images/{image}'  # Adjust this path\n",
    "        dst_path = f'{directory_path}/{image}'\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print(f'Copied {len(images_to_copy)} images for dog ID {dog_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('dataset_with_morethan_6imgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Path to the original dataset\n",
    "original_dataset_path = 'dataset_with_morethan_6imgs_split'\n",
    "\n",
    "# Path to the new sub-dataset\n",
    "sub_dataset_path = '101_classes_dataset_for_testing'\n",
    "\n",
    "# Number of classes you want in the sub-dataset\n",
    "num_classes = 101\n",
    "\n",
    "# Train and validation directories\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "\n",
    "def create_sub_dataset(original_path, sub_path, num_classes):\n",
    "    # Get all class names\n",
    "    classes = os.listdir(os.path.join(original_path, train_dir))\n",
    "    \n",
    "    # Shuffle and select the desired number of classes\n",
    "    selected_classes = random.sample(classes, num_classes)\n",
    "    \n",
    "    # Create train and val directories in the sub-dataset path\n",
    "    os.makedirs(os.path.join(sub_path, train_dir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(sub_path, val_dir), exist_ok=True)\n",
    "\n",
    "    # Move the selected classes' images to the new sub-dataset\n",
    "    for cls in selected_classes:\n",
    "        shutil.move(os.path.join(original_path, train_dir, cls), os.path.join(sub_path, train_dir, cls))\n",
    "        shutil.move(os.path.join(original_path, val_dir, cls), os.path.join(sub_path, val_dir, cls))\n",
    "\n",
    "create_sub_dataset(original_dataset_path, sub_dataset_path, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset_dir = 'dataset_with_morethan_6imgs'\n",
    "train_dir = 'dataset_with_morethan_6imgs_split/train'\n",
    "val_dir = 'dataset_with_morethan_6imgs_split/val'\n",
    "# test_dir = 'dataset2/test'\n",
    "\n",
    "# Create training, validation, and test directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Specify the percentage for validation and test data\n",
    "validation_split = 0.2\n",
    "# test_split = 0.1\n",
    "\n",
    "for class_folder in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_folder)\n",
    "    train_class_path = os.path.join(train_dir, class_folder)\n",
    "    val_class_path = os.path.join(val_dir, class_folder)\n",
    "    # test_class_path = os.path.join(test_dir, class_folder)\n",
    "\n",
    "    os.makedirs(train_class_path, exist_ok=True)\n",
    "    os.makedirs(val_class_path, exist_ok=True)\n",
    "    # os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "    # List all the image files in the class folder\n",
    "    images = os.listdir(class_path)\n",
    "\n",
    "    # Calculate the number of images for validation and test sets\n",
    "    num_validation = int(validation_split * len(images))\n",
    "    # num_test = int(test_split * len(images))\n",
    "\n",
    "    # if you want to just copy the files you may use copy instead of move\n",
    "    # Move images to the validation directory \n",
    "    for img in images[:num_validation]:\n",
    "        src = os.path.join(class_path, img)\n",
    "        dest = os.path.join(val_class_path, img)\n",
    "        shutil.copy(src, dest)\n",
    "\n",
    "    # Move images to the test directory\n",
    "    # for img in images[num_validation:num_validation + num_test]:\n",
    "    #     src = os.path.join(class_path, img)\n",
    "    #     dest = os.path.join(test_class_path, img)\n",
    "    #     shutil.move(src, dest)\n",
    "\n",
    "    # Move the remaining images to the training directory\n",
    "    for img in images[num_validation:]:\n",
    "        src = os.path.join(class_path, img)\n",
    "        dest = os.path.join(train_class_path, img)\n",
    "        shutil.copy(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import combinations\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DogNoseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Directory with all the subdirectories of classes.\n",
    "        transform: Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []  # Store image paths\n",
    "        self.labels = []       # Store class labels for each image\n",
    "\n",
    "        # Walk through the directory to list all images\n",
    "        for class_id, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_filename in os.listdir(class_dir):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_filename))\n",
    "                    self.labels.append(class_name)\n",
    "\n",
    "        # Create positive and negative pairs\n",
    "        self.pairs, self.pair_labels = self._create_pairs()\n",
    "\n",
    "    def _create_pairs(self):\n",
    "        from collections import defaultdict\n",
    "        # Dictionary to hold class specific images\n",
    "        class_dict = defaultdict(list)\n",
    "        for path, label in zip(self.image_paths, self.labels):\n",
    "            class_dict[label].append(path)\n",
    "\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "\n",
    "        # Create positive pairs within the same class\n",
    "        for class_ID,paths in class_dict.items():\n",
    "            arr=list(combinations(paths, 2))\n",
    "            # positive_pairs=[]\n",
    "            for val in arr:\n",
    "                positive_pairs.append([val[0],val[1],class_ID,class_ID])\n",
    "            # print(len(positive_pairs))\n",
    "            # positive_pairs.extend(list(combinations(paths, 2)))\n",
    "\n",
    "        # Create negative pairs between classes\n",
    "        unique_labels = list(class_dict.keys())\n",
    "        for i in range(len(unique_labels)):\n",
    "            for j in range(i + 1, len(unique_labels)):\n",
    "                for a in class_dict[unique_labels[i]]:\n",
    "                    for b in class_dict[unique_labels[j]]:\n",
    "                        negative_pairs.append([a, b,unique_labels[i],unique_labels[j]])\n",
    "                        # negative_pairs.append((b, a))  # Ensure symmetry\n",
    "\n",
    "        # Randomly select negative pairs to match number of positive pairs\n",
    "        random.shuffle(negative_pairs)\n",
    "        negative_pairs = negative_pairs[:len(positive_pairs)]\n",
    "\n",
    "        # Combine and label pairs: 1 for positive, 0 for negative\n",
    "        pairs = positive_pairs + negative_pairs\n",
    "        labels = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n",
    "\n",
    "        # Shuffle pairs to mix positive and negative\n",
    "        combined = list(zip(pairs, labels))\n",
    "        random.shuffle(combined)\n",
    "        # print(len(combined))\n",
    "        return zip(*combined)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (img1_path, img2_path,class_Id1,class_Id2), label = self.pairs[index], self.pair_labels[index]\n",
    "        # print(img1_path, img2_path,class_Id1,class_Id2,label)\n",
    "        img1, img2 = Image.open(img1_path), Image.open(img2_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return (img1, img2,class_Id1,class_Id2), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, dummy_image_path=\"101_classes_dataset_for_testing/db_images/1771/A*d-fSRoB7LOAAAAAAAAAAAAAAAQAAAQ.jpg\"):\n",
    "        \"\"\"\n",
    "        root_dir: Directory with all the subdirectories of classes.\n",
    "        transform: Optional transform to be applied on a sample.\n",
    "        dummy_image_path: Path to a dummy image used to fill the second image slot.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.dummy_image_path = dummy_image_path\n",
    "        self.image_paths = []  # Store image paths\n",
    "        self.labels = []       # Store class labels for each image\n",
    "\n",
    "        # Walk through the directory to list all images\n",
    "        for class_id, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_filename in os.listdir(class_dir):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_filename))\n",
    "                    self.labels.append(class_name)  # Store numeric class ID for consistency\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        real_img_path, label = self.image_paths[index], self.labels[index]\n",
    "        real_img = Image.open(real_img_path)\n",
    "        dummy_img = Image.open(self.dummy_image_path)  # Load the dummy image\n",
    "\n",
    "        if self.transform:\n",
    "            real_img = self.transform(real_img)\n",
    "            dummy_img = self.transform(dummy_img)\n",
    "\n",
    "        return (real_img, dummy_img), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dir = '400_classes_dataset/train'\n",
    "db_dir = '101_classes_dataset_for_testing/db_images'\n",
    "q_dir = '101_classes_dataset_for_testing/query_images'\n",
    "train_dataset = DogNoseDataset(root_dir=train_dir, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "validation_dataset = DogNoseDataset(root_dir=db_dir, transform=transform)\n",
    "# Assuming you have a separate validation dataloader\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = QueryDataset(root_dir=q_dir, transform=transform)\n",
    "# Assuming you have a separate validation dataloader\n",
    "query_dataloader = DataLoader(query_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2443"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1266"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iter(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[[[0.4157, 0.4118, 0.4118,  ..., 0.4902, 0.5020, 0.5098],\n",
       "            [0.4000, 0.3961, 0.3961,  ..., 0.5176, 0.5255, 0.5294],\n",
       "            [0.3922, 0.3882, 0.3843,  ..., 0.5333, 0.5373, 0.5373],\n",
       "            ...,\n",
       "            [0.3843, 0.3882, 0.4039,  ..., 0.7059, 0.7255, 0.7412],\n",
       "            [0.3608, 0.3686, 0.3882,  ..., 0.7098, 0.7294, 0.7451],\n",
       "            [0.3412, 0.3529, 0.3765,  ..., 0.7137, 0.7333, 0.7490]],\n",
       "  \n",
       "           [[0.3137, 0.3098, 0.3098,  ..., 0.3647, 0.3765, 0.3843],\n",
       "            [0.2980, 0.2941, 0.2941,  ..., 0.3922, 0.4000, 0.4039],\n",
       "            [0.2902, 0.2863, 0.2824,  ..., 0.4078, 0.4118, 0.4118],\n",
       "            ...,\n",
       "            [0.3451, 0.3490, 0.3647,  ..., 0.6431, 0.6627, 0.6745],\n",
       "            [0.3216, 0.3294, 0.3490,  ..., 0.6431, 0.6627, 0.6784],\n",
       "            [0.3020, 0.3137, 0.3412,  ..., 0.6471, 0.6667, 0.6824]],\n",
       "  \n",
       "           [[0.2235, 0.2196, 0.2196,  ..., 0.2157, 0.2275, 0.2353],\n",
       "            [0.2078, 0.2039, 0.2039,  ..., 0.2431, 0.2510, 0.2549],\n",
       "            [0.2000, 0.1961, 0.1922,  ..., 0.2588, 0.2627, 0.2627],\n",
       "            ...,\n",
       "            [0.2980, 0.3020, 0.3176,  ..., 0.5529, 0.5765, 0.5922],\n",
       "            [0.2745, 0.2863, 0.3059,  ..., 0.5569, 0.5843, 0.6000],\n",
       "            [0.2549, 0.2706, 0.2980,  ..., 0.5647, 0.5882, 0.6039]]],\n",
       "  \n",
       "  \n",
       "          [[[0.6667, 0.6667, 0.6745,  ..., 0.7765, 0.7843, 0.7961],\n",
       "            [0.6588, 0.6627, 0.6667,  ..., 0.7765, 0.7843, 0.8000],\n",
       "            [0.6588, 0.6627, 0.6627,  ..., 0.7882, 0.7882, 0.8000],\n",
       "            ...,\n",
       "            [0.3922, 0.3843, 0.3843,  ..., 0.5333, 0.5373, 0.5451],\n",
       "            [0.4039, 0.3961, 0.3961,  ..., 0.5294, 0.5255, 0.5333],\n",
       "            [0.4039, 0.4000, 0.4000,  ..., 0.5333, 0.5294, 0.5255]],\n",
       "  \n",
       "           [[0.6471, 0.6471, 0.6588,  ..., 0.6275, 0.6314, 0.6314],\n",
       "            [0.6471, 0.6510, 0.6549,  ..., 0.6275, 0.6314, 0.6314],\n",
       "            [0.6471, 0.6510, 0.6510,  ..., 0.6314, 0.6314, 0.6314],\n",
       "            ...,\n",
       "            [0.3569, 0.3490, 0.3451,  ..., 0.3725, 0.3765, 0.3765],\n",
       "            [0.3686, 0.3608, 0.3569,  ..., 0.3686, 0.3647, 0.3647],\n",
       "            [0.3647, 0.3608, 0.3608,  ..., 0.3686, 0.3647, 0.3608]],\n",
       "  \n",
       "           [[0.7333, 0.7255, 0.7294,  ..., 0.5529, 0.5490, 0.5451],\n",
       "            [0.7255, 0.7255, 0.7255,  ..., 0.5529, 0.5569, 0.5529],\n",
       "            [0.7216, 0.7255, 0.7216,  ..., 0.5569, 0.5608, 0.5569],\n",
       "            ...,\n",
       "            [0.3294, 0.3216, 0.3176,  ..., 0.2863, 0.2941, 0.3020],\n",
       "            [0.3373, 0.3333, 0.3294,  ..., 0.2824, 0.2824, 0.2863],\n",
       "            [0.3294, 0.3255, 0.3255,  ..., 0.2824, 0.2784, 0.2745]]],\n",
       "  \n",
       "  \n",
       "          [[[0.9569, 0.9569, 0.9608,  ..., 0.9059, 0.8980, 0.8941],\n",
       "            [0.9529, 0.9529, 0.9569,  ..., 0.9059, 0.8980, 0.8941],\n",
       "            [0.9490, 0.9490, 0.9529,  ..., 0.9098, 0.9020, 0.8980],\n",
       "            ...,\n",
       "            [0.2196, 0.2235, 0.2275,  ..., 0.4471, 0.4667, 0.4784],\n",
       "            [0.2275, 0.2314, 0.2353,  ..., 0.4471, 0.4667, 0.4745],\n",
       "            [0.2314, 0.2353, 0.2392,  ..., 0.4471, 0.4667, 0.4745]],\n",
       "  \n",
       "           [[0.9333, 0.9333, 0.9373,  ..., 0.8706, 0.8627, 0.8588],\n",
       "            [0.9294, 0.9294, 0.9333,  ..., 0.8706, 0.8627, 0.8588],\n",
       "            [0.9255, 0.9255, 0.9294,  ..., 0.8745, 0.8667, 0.8627],\n",
       "            ...,\n",
       "            [0.1373, 0.1412, 0.1451,  ..., 0.2627, 0.2784, 0.2863],\n",
       "            [0.1451, 0.1490, 0.1529,  ..., 0.2667, 0.2745, 0.2824],\n",
       "            [0.1490, 0.1529, 0.1569,  ..., 0.2667, 0.2745, 0.2824]],\n",
       "  \n",
       "           [[0.9333, 0.9333, 0.9373,  ..., 0.8039, 0.7961, 0.7922],\n",
       "            [0.9294, 0.9294, 0.9333,  ..., 0.8039, 0.7961, 0.7922],\n",
       "            [0.9255, 0.9255, 0.9294,  ..., 0.8078, 0.8000, 0.7961],\n",
       "            ...,\n",
       "            [0.0314, 0.0353, 0.0392,  ..., 0.0275, 0.0471, 0.0549],\n",
       "            [0.0392, 0.0431, 0.0471,  ..., 0.0314, 0.0431, 0.0510],\n",
       "            [0.0431, 0.0471, 0.0510,  ..., 0.0314, 0.0431, 0.0510]]],\n",
       "  \n",
       "  \n",
       "          [[[0.7333, 0.7294, 0.7333,  ..., 0.5961, 0.6000, 0.6039],\n",
       "            [0.7333, 0.7333, 0.7333,  ..., 0.6000, 0.6078, 0.6118],\n",
       "            [0.7373, 0.7373, 0.7373,  ..., 0.6039, 0.6078, 0.6118],\n",
       "            ...,\n",
       "            [0.6627, 0.6588, 0.6431,  ..., 0.6510, 0.6392, 0.6235],\n",
       "            [0.6706, 0.6627, 0.6510,  ..., 0.6431, 0.6549, 0.6510],\n",
       "            [0.6667, 0.6706, 0.6627,  ..., 0.5961, 0.6431, 0.6706]],\n",
       "  \n",
       "           [[0.7020, 0.6980, 0.6902,  ..., 0.5529, 0.5569, 0.5608],\n",
       "            [0.7020, 0.7020, 0.7020,  ..., 0.5569, 0.5647, 0.5686],\n",
       "            [0.7059, 0.7059, 0.7059,  ..., 0.5608, 0.5647, 0.5686],\n",
       "            ...,\n",
       "            [0.6706, 0.6706, 0.6588,  ..., 0.6784, 0.6706, 0.6549],\n",
       "            [0.6784, 0.6745, 0.6627,  ..., 0.6706, 0.6863, 0.6824],\n",
       "            [0.6745, 0.6784, 0.6745,  ..., 0.6275, 0.6745, 0.7020]],\n",
       "  \n",
       "           [[0.6118, 0.6078, 0.6039,  ..., 0.4824, 0.4863, 0.4902],\n",
       "            [0.6196, 0.6196, 0.6196,  ..., 0.4863, 0.4941, 0.4980],\n",
       "            [0.6353, 0.6353, 0.6275,  ..., 0.4902, 0.4941, 0.4980],\n",
       "            ...,\n",
       "            [0.7216, 0.7216, 0.7098,  ..., 0.7216, 0.7098, 0.6941],\n",
       "            [0.7294, 0.7255, 0.7216,  ..., 0.7137, 0.7294, 0.7255],\n",
       "            [0.7255, 0.7294, 0.7333,  ..., 0.6706, 0.7176, 0.7451]]]]),\n",
       "  tensor([[[[0.2902, 0.3922, 0.3647,  ..., 0.5294, 0.5804, 0.5843],\n",
       "            [0.3176, 0.4235, 0.4118,  ..., 0.5686, 0.6118, 0.6314],\n",
       "            [0.3412, 0.3922, 0.4039,  ..., 0.6039, 0.6196, 0.6157],\n",
       "            ...,\n",
       "            [0.4314, 0.4392, 0.3843,  ..., 0.3059, 0.2980, 0.2706],\n",
       "            [0.4235, 0.4157, 0.3686,  ..., 0.3098, 0.3137, 0.2902],\n",
       "            [0.4118, 0.3882, 0.3451,  ..., 0.3137, 0.3255, 0.3020]],\n",
       "  \n",
       "           [[0.2667, 0.3608, 0.3333,  ..., 0.4863, 0.5373, 0.5412],\n",
       "            [0.2941, 0.3922, 0.3804,  ..., 0.5255, 0.5686, 0.5882],\n",
       "            [0.3137, 0.3647, 0.3765,  ..., 0.5608, 0.5765, 0.5725],\n",
       "            ...,\n",
       "            [0.3686, 0.3765, 0.3255,  ..., 0.2549, 0.2510, 0.2196],\n",
       "            [0.3608, 0.3490, 0.3020,  ..., 0.2549, 0.2588, 0.2353],\n",
       "            [0.3490, 0.3255, 0.2824,  ..., 0.2510, 0.2627, 0.2510]],\n",
       "  \n",
       "           [[0.1725, 0.2745, 0.2510,  ..., 0.4078, 0.4588, 0.4627],\n",
       "            [0.2039, 0.3059, 0.2980,  ..., 0.4471, 0.4902, 0.5098],\n",
       "            [0.2353, 0.2863, 0.2980,  ..., 0.4824, 0.4980, 0.4941],\n",
       "            ...,\n",
       "            [0.2784, 0.2941, 0.2549,  ..., 0.1961, 0.1922, 0.1647],\n",
       "            [0.2706, 0.2667, 0.2353,  ..., 0.1922, 0.2000, 0.1804],\n",
       "            [0.2588, 0.2431, 0.2118,  ..., 0.1922, 0.2039, 0.1882]]],\n",
       "  \n",
       "  \n",
       "          [[[0.6471, 0.6431, 0.6392,  ..., 0.7529, 0.7569, 0.7647],\n",
       "            [0.6431, 0.6392, 0.6353,  ..., 0.7686, 0.7725, 0.7725],\n",
       "            [0.6431, 0.6353, 0.6314,  ..., 0.7804, 0.7804, 0.7765],\n",
       "            ...,\n",
       "            [0.4314, 0.4275, 0.4235,  ..., 0.4941, 0.4980, 0.5020],\n",
       "            [0.4392, 0.4314, 0.4235,  ..., 0.4941, 0.4980, 0.4980],\n",
       "            [0.4471, 0.4353, 0.4275,  ..., 0.4980, 0.5020, 0.5020]],\n",
       "  \n",
       "           [[0.6706, 0.6667, 0.6627,  ..., 0.6078, 0.6118, 0.6196],\n",
       "            [0.6667, 0.6627, 0.6588,  ..., 0.6235, 0.6275, 0.6275],\n",
       "            [0.6706, 0.6627, 0.6588,  ..., 0.6353, 0.6353, 0.6314],\n",
       "            ...,\n",
       "            [0.4078, 0.4039, 0.3961,  ..., 0.3333, 0.3373, 0.3412],\n",
       "            [0.4196, 0.4118, 0.4039,  ..., 0.3333, 0.3373, 0.3373],\n",
       "            [0.4275, 0.4157, 0.4078,  ..., 0.3373, 0.3412, 0.3412]],\n",
       "  \n",
       "           [[0.7176, 0.7137, 0.7098,  ..., 0.5059, 0.5098, 0.5176],\n",
       "            [0.7137, 0.7098, 0.7059,  ..., 0.5216, 0.5255, 0.5255],\n",
       "            [0.7176, 0.7137, 0.7059,  ..., 0.5333, 0.5333, 0.5294],\n",
       "            ...,\n",
       "            [0.3843, 0.3804, 0.3725,  ..., 0.2549, 0.2549, 0.2549],\n",
       "            [0.3961, 0.3882, 0.3804,  ..., 0.2549, 0.2549, 0.2510],\n",
       "            [0.4039, 0.3922, 0.3843,  ..., 0.2588, 0.2588, 0.2549]]],\n",
       "  \n",
       "  \n",
       "          [[[0.8667, 0.8627, 0.8549,  ..., 0.4980, 0.4902, 0.4863],\n",
       "            [0.8667, 0.8627, 0.8549,  ..., 0.4980, 0.4902, 0.4863],\n",
       "            [0.8706, 0.8667, 0.8588,  ..., 0.4941, 0.4863, 0.4824],\n",
       "            ...,\n",
       "            [0.7569, 0.7608, 0.7686,  ..., 0.7176, 0.7137, 0.7137],\n",
       "            [0.7569, 0.7608, 0.7686,  ..., 0.7137, 0.7098, 0.7098],\n",
       "            [0.7569, 0.7608, 0.7686,  ..., 0.7137, 0.7098, 0.7098]],\n",
       "  \n",
       "           [[0.8824, 0.8784, 0.8706,  ..., 0.4157, 0.4078, 0.4039],\n",
       "            [0.8824, 0.8784, 0.8706,  ..., 0.4157, 0.4078, 0.4039],\n",
       "            [0.8784, 0.8745, 0.8667,  ..., 0.4118, 0.4039, 0.4000],\n",
       "            ...,\n",
       "            [0.7176, 0.7216, 0.7294,  ..., 0.5961, 0.5922, 0.5922],\n",
       "            [0.7176, 0.7216, 0.7294,  ..., 0.5922, 0.5882, 0.5882],\n",
       "            [0.7176, 0.7216, 0.7294,  ..., 0.5922, 0.5882, 0.5882]],\n",
       "  \n",
       "           [[0.8863, 0.8824, 0.8745,  ..., 0.4039, 0.3961, 0.3922],\n",
       "            [0.8863, 0.8824, 0.8745,  ..., 0.4039, 0.3961, 0.3922],\n",
       "            [0.8863, 0.8824, 0.8745,  ..., 0.4000, 0.3922, 0.3882],\n",
       "            ...,\n",
       "            [0.7098, 0.7137, 0.7216,  ..., 0.5882, 0.5843, 0.5843],\n",
       "            [0.7098, 0.7137, 0.7216,  ..., 0.5843, 0.5804, 0.5804],\n",
       "            [0.7098, 0.7137, 0.7216,  ..., 0.5843, 0.5804, 0.5804]]],\n",
       "  \n",
       "  \n",
       "          [[[0.7804, 0.7725, 0.7647,  ..., 0.6000, 0.6000, 0.6078],\n",
       "            [0.7922, 0.7882, 0.7804,  ..., 0.6039, 0.6157, 0.6196],\n",
       "            [0.7922, 0.7882, 0.7882,  ..., 0.6196, 0.6314, 0.6314],\n",
       "            ...,\n",
       "            [0.6314, 0.6941, 0.7294,  ..., 0.6784, 0.6588, 0.6588],\n",
       "            [0.6627, 0.7098, 0.7412,  ..., 0.6784, 0.6706, 0.6706],\n",
       "            [0.6510, 0.6902, 0.7216,  ..., 0.6627, 0.6706, 0.6706]],\n",
       "  \n",
       "           [[0.7176, 0.7098, 0.7059,  ..., 0.5294, 0.5294, 0.5373],\n",
       "            [0.7294, 0.7216, 0.7216,  ..., 0.5333, 0.5451, 0.5490],\n",
       "            [0.7333, 0.7294, 0.7294,  ..., 0.5569, 0.5608, 0.5608],\n",
       "            ...,\n",
       "            [0.6196, 0.6824, 0.7176,  ..., 0.6902, 0.6706, 0.6706],\n",
       "            [0.6510, 0.6980, 0.7294,  ..., 0.6902, 0.6824, 0.6824],\n",
       "            [0.6314, 0.6784, 0.7098,  ..., 0.6745, 0.6824, 0.6824]],\n",
       "  \n",
       "           [[0.6275, 0.6196, 0.6235,  ..., 0.4745, 0.4745, 0.4824],\n",
       "            [0.6392, 0.6431, 0.6392,  ..., 0.4784, 0.4902, 0.4941],\n",
       "            [0.6510, 0.6471, 0.6549,  ..., 0.4941, 0.5059, 0.5059],\n",
       "            ...,\n",
       "            [0.6941, 0.7569, 0.8000,  ..., 0.7255, 0.7059, 0.7059],\n",
       "            [0.7255, 0.7725, 0.8078,  ..., 0.7255, 0.7176, 0.7176],\n",
       "            [0.7098, 0.7529, 0.7882,  ..., 0.7098, 0.7176, 0.7176]]]]),\n",
       "  ('5975', '5102', '5107', '5101'),\n",
       "  ('5977', '5102', '5152', '5101')],\n",
       " tensor([0, 1, 0, 1])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, s=64.0, m=0.5):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_classes, embedding_size)).to(device)\n",
    "        self.reset_parameters()\n",
    "        self.cos_m = torch.cos(torch.tensor(m, device=self.weight.device))  # Set device during initialization\n",
    "        self.sin_m = torch.sin(torch.tensor(m, device=self.weight.device))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        # Ensure input is on the same device as model\n",
    "        input = input.to(self.weight.device)\n",
    "        labels = labels.to(self.weight.device)\n",
    "        \n",
    "        # Normalize feature vectors and weight vectors to get only angles\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        \n",
    "        # cos(theta + m) calculation\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > 0, phi, cosine)\n",
    "        \n",
    "        # Add margin only to the correct class\n",
    "        one_hot = torch.zeros(cosine.size(), device=self.weight.device)  # Specify device here\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        \n",
    "        # Cross entropy loss\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# class ArcFaceLoss(nn.Module):\n",
    "#     def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "#         \"\"\"\n",
    "#         ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "#         (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "#         Args:\n",
    "#             num_classes: The number of classes in your training dataset\n",
    "#             embedding_size: The size of the embeddings that you pass into\n",
    "#             margin: m in the paper, the angular margin penalty in radians\n",
    "#             scale: s in the paper, feature scale\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.margin = margin\n",
    "#         self.scale = scale\n",
    "#         self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size).to(device))\n",
    "#         nn.init.xavier_normal_(self.W)\n",
    "\n",
    "#     def forward(self, embeddings, labels):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             embeddings: (None, embedding_size)\n",
    "#             labels: (None,)\n",
    "#         Returns:\n",
    "#             loss: scalar\n",
    "#         \"\"\"\n",
    "#         cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "#         mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "#         cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "#         modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "#             cosine_of_target_classes\n",
    "#         ) # (None, )\n",
    "#         diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "#         logits = cosine + (mask * diff) # (None, n_classes)\n",
    "#         logits = self.scale_logits(logits) # (None, n_classes)\n",
    "#         return nn.CrossEntropyLoss()(logits, labels)\n",
    "    \n",
    "#     def get_cosine(self, embeddings):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             embeddings: (None, embedding_size)\n",
    "#         Returns:\n",
    "#             cosine: (None, n_classes)\n",
    "#         \"\"\"\n",
    "#         cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "#         return cosine\n",
    "    \n",
    "#     def get_target_mask(self, labels):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             labels: (None,)\n",
    "#         Returns:\n",
    "#             mask: (None, n_classes)\n",
    "#         \"\"\"\n",
    "#         batch_size = labels.size(0)\n",
    "#         onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "#         onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "#         return onehot\n",
    "    \n",
    "#     def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             cosine_of_target_classes: (None,)\n",
    "#         Returns:\n",
    "#             modified_cosine_of_target_classes: (None,)\n",
    "#         \"\"\"\n",
    "#         eps = 1e-6\n",
    "#         # theta in the paper\n",
    "#         angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "#         return torch.cos(angles + self.margin)\n",
    "    \n",
    "#     def scale_logits(self, logits):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             logits: (None, n_classes)\n",
    "#         Returns:\n",
    "#             scaled_logits: (None, n_classes)\n",
    "#         \"\"\"\n",
    "#         return logits * self.scale\n",
    "    \n",
    "# Set parameters\n",
    "# num_classes = 10\n",
    "# embedding_size = 1024\n",
    "# margin = 0.5\n",
    "# scale = 30.0\n",
    "\n",
    "# # Create dummy input data\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# embeddings = torch.randn((16, embedding_size)).to(device)  # Batch size of 16\n",
    "# labels = torch.randint(0, num_classes, (16,)).to(device)   # Random labels for the batch\n",
    "\n",
    "# # Instantiate ArcFaceLoss\n",
    "# arcface_loss = ArcFaceLoss(num_classes, embedding_size, margin, scale).to(device)\n",
    "\n",
    "# # Calculate the loss\n",
    "# loss = arcface_loss(embeddings, labels)\n",
    "\n",
    "# # Print the loss\n",
    "# print(\"Calculated loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x1, x2, labels):\n",
    "        # Calculate Euclidean distance between x1 and x2\n",
    "        distances = F.pairwise_distance(x1, x2, keepdim=True)\n",
    "        \n",
    "        # Calculate Contrastive loss\n",
    "        losses = (1 - labels) * torch.pow(torch.clamp(self.margin - distances, min=0.0), 2) + \\\n",
    "                 labels * torch.pow(distances, 2)\n",
    "\n",
    "        return losses.mean()\n",
    "\n",
    "# Example usage:\n",
    "# margin = 1.0\n",
    "# criterion = ContrastiveLoss(margin)\n",
    "# loss = criterion(embeddings_anchor, embeddings_pair, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        # Load a pre-trained ResNet-152 and remove the last GAP and FC\n",
    "        base_model = resnet152(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "        # Additional blocks to reduce channel dimensions\n",
    "        self.reduce_channels = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, kernel_size=3),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 512, kernel_size=3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.reduce_channels(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input feature map with shape (batch_size, C, H, W)\n",
    "        batch_size, C, H, W = x.size()\n",
    "        \n",
    "        # Reshape x to (batch_size, C, H*W)\n",
    "        x_reshaped = x.view(batch_size, C, -1)\n",
    "        \n",
    "        # Compute the channel attention map\n",
    "        channel_attention_map = torch.bmm(x_reshaped, x_reshaped.transpose(1, 2))\n",
    "        channel_attention_map = F.softmax(channel_attention_map, dim=1)\n",
    "        \n",
    "        # Multiply the attention map by the input feature map\n",
    "        x_weighted = torch.bmm(channel_attention_map, x_reshaped)\n",
    "        \n",
    "        # Reshape back to (batch_size, C, H, W)\n",
    "        x_weighted = x_weighted.view(batch_size, C, H, W)\n",
    "        \n",
    "        # Apply scale parameter and element-wise summation\n",
    "        beta = torch.nn.Parameter(torch.zeros(1)).to(device)\n",
    "        out = beta * x_weighted + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input feature map with shape (batch_size, C, H, W)\n",
    "        batch_size, Ch, H, W = x.size()\n",
    "        \n",
    "        # Obtain new feature maps B and C\n",
    "        B = self.conv1(x)\n",
    "        C = self.conv2(x)\n",
    "        \n",
    "        # Reshape B and C to (batch_size, C, H*W)\n",
    "        B_reshaped = B.view(batch_size, Ch, -1)\n",
    "        C_reshaped = C.view(batch_size, Ch, -1)\n",
    "        \n",
    "        # Compute the spatial attention map\n",
    "        spatial_attention_map = torch.bmm(B_reshaped.transpose(1, 2), C_reshaped)\n",
    "        spatial_attention_map = F.softmax(spatial_attention_map, dim=1)\n",
    "        \n",
    "        # Multiply the attention map by the input feature map\n",
    "        D = self.conv3(x)\n",
    "        D_reshaped = D.view(batch_size, Ch, -1)\n",
    "        x_weighted = torch.bmm(spatial_attention_map, D_reshaped.transpose(1, 2))\n",
    "        \n",
    "        # Reshape back to (batch_size, C, H, W)\n",
    "        x_weighted = x_weighted.view(batch_size, Ch, H, W)\n",
    "        \n",
    "        # Apply scale parameter and element-wise summation\n",
    "        alpha = torch.nn.Parameter(torch.zeros(1)).to(device)\n",
    "        out = alpha * x_weighted + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "class DualAttentionNetwork(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(DualAttentionNetwork, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels)\n",
    "        self.spatial_attention = SpatialAttention(in_channels)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(in_channels * 3, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply channel attention\n",
    "        channel_attention_map = self.channel_attention(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        spatial_attention_map = self.spatial_attention(x)\n",
    "        \n",
    "        # Concatenate the input feature map with channel and spatial attention maps\n",
    "        concatenated = torch.cat([x, channel_attention_map, spatial_attention_map], dim=1)\n",
    "        \n",
    "        # Global average pooling\n",
    "        gap = self.global_avg_pool(concatenated)\n",
    "        gap = gap.view(gap.size(0), -1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(gap)\n",
    "        \n",
    "        return out\n",
    "        # return concatenated\n",
    "\n",
    "# Example usage\n",
    "# in_channels = 512\n",
    "# input_tensor = torch.randn((1, in_channels, 32, 32)).to(device)  # Example input tensor\n",
    "\n",
    "# dan = DualAttentionNetwork(in_channels).to(device)\n",
    "# output = dan(input_tensor)\n",
    "# print(output.shape)  # Should be (8, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = ResNetBackbone()\n",
    "        self.attention = DualAttentionNetwork(512)\n",
    "        # self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        # self.fc = nn.Linear(512, 1024)  # Output 1024-dimensional embedding\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.backbone(x1)\n",
    "        out1 = self.attention(out1)\n",
    "        # out1 = self.pooling(out1)\n",
    "        # out1 = out1.view(out1.size(0), -1)\n",
    "        # out1 = self.fc(out1)\n",
    "\n",
    "        out2 = self.backbone(x2)\n",
    "        out2 = self.attention(out2)\n",
    "        # out2 = self.pooling(out2)\n",
    "        # out2 = out2.view(out2.size(0), -1)\n",
    "        # out2 = self.fc(out2)\n",
    "\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athena/anaconda3/envs/lego/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/athena/anaconda3/envs/lego/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "optimizer_contrastive = optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_arcface = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "criterion_contrastive = ContrastiveLoss(margin=2.0)\n",
    "criterion_arcface = ArcFaceLoss(num_classes=num_classes,embedding_size=1024, m=0.5, s=30.0)\n",
    "# criterion_arcface = ArcFaceLoss(num_classes=num_classes, margin=0.5, scale=30.0)\n",
    "\n",
    "# # Scheduler for learning rate decay of the ArcFace optimizer\n",
    "# scheduler_arcface = optim.lr_scheduler.LinearLR(optimizer_arcface, start_factor=1.0, end_factor=0, total_iters=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, query_dataloader, db_embeddings, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for [img1, img2], labels in query_dataloader:\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            query_output, _ = model(img1, img2)\n",
    "            query_emb = query_output.detach().cpu()\n",
    "            \n",
    "            min_distance = float('inf')\n",
    "            predicted_class = None\n",
    "            \n",
    "            # Compare with each db_embeddings\n",
    "            for db_class_id, embeddings in db_embeddings.items():\n",
    "                for emb in embeddings:\n",
    "                    distance = F.pairwise_distance(query_emb, emb).item()\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        predicted_class = db_class_id\n",
    "            \n",
    "            # print(\"comparing: \",predicted_class, '---', labels[0])\n",
    "            # print(\"Count: \", count)\n",
    "            # Check if the prediction is correct\n",
    "            if predicted_class == labels[0]:\n",
    "                correct += 1\n",
    "            # else:\n",
    "            #     print(predicted_class, '-------', labels[0])\n",
    "            total += 1\n",
    "    \n",
    "    print(\"Total, correct: \", total, correct)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scheduler_arcface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     optimizer_contrastive\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m     optimizer_arcface\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mscheduler_arcface\u001b[49m\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the learning rate\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print(f\"Training ended for epoch {epoch+1} with the loss: {train_loss/len(train_dataloader)}\")\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scheduler_arcface' is not defined"
     ]
    }
   ],
   "source": [
    "best_acc = -1\n",
    "# Training loop\n",
    "for epoch in range(50):  # Assuming you mistakenly mentioned 100 epochs in the comment.\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    validation_embeddings = {}\n",
    "    train_loss = 0.0\n",
    "    for [img1, img2, class_id1, class_id2], labels in train_dataloader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "        output1, output2 = model(img1, img2)\n",
    "        # euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "        # Compute Contrastive Loss\n",
    "        loss_contrastive = criterion_contrastive(output1, output2, labels)\n",
    "        \n",
    "        # Calculate the mean embedding for ArcFace Loss (simplified version)\n",
    "        embeddings_anchor = output1.to(device)\n",
    "        embeddings_pair = output2.to(device)\n",
    "        loss_arcface_anchor = criterion_arcface(embeddings_anchor, labels)\n",
    "        loss_arcface_pair = criterion_arcface(embeddings_pair, labels)\n",
    "        loss_arcface = 0.5 * (loss_arcface_anchor + loss_arcface_pair)\n",
    "       \n",
    "        # print(\"Train: \",\"Loss Contrastive: \",loss_contrastive, \"Loss ArcFace: \", loss_arcface)\n",
    "        # Combine losses\n",
    "        total_loss = loss_contrastive + loss_arcface\n",
    "        train_loss += total_loss.item()\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer_contrastive.zero_grad()\n",
    "        optimizer_arcface.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer_contrastive.step()\n",
    "        optimizer_arcface.step()\n",
    "    \n",
    "    scheduler_arcface.step()  # Update the learning rate\n",
    "\n",
    "    # print(f\"Training ended for epoch {epoch+1} with the loss: {train_loss/len(train_dataloader)}\")\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Gradient computation is not needed in validation\n",
    "        val_loss = 0.0\n",
    "        print(f\"Validation started for epoch {epoch + 1}....\")\n",
    "        for [img1, img2, class_id1, class_id2], labels in validation_dataloader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "            # euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_contrastive = criterion_contrastive(output1, output2, labels)\n",
    "            embeddings_anchor = output1\n",
    "            embeddings_pair = output2\n",
    "            loss_arcface_anchor = criterion_arcface(embeddings_anchor, labels)\n",
    "            loss_arcface_pair = criterion_arcface(embeddings_pair, labels)\n",
    "            loss_arcface = 0.5 * (loss_arcface_anchor + loss_arcface_pair)\n",
    "\n",
    "            # print(\"Val: \",\"Loss Contrastive: \",loss_contrastive, \"Loss ArcFace: \", loss_arcface)\n",
    "\n",
    "            # Combine losses\n",
    "            val_total_loss = loss_contrastive + loss_arcface\n",
    "\n",
    "            val_loss += val_total_loss.item()  # Accumulate the validation loss\n",
    "\n",
    "                    # Update embeddings dictionary\n",
    "            for i in range(len(class_id1)):\n",
    "                cid1 = class_id1[i]\n",
    "                cid2 = class_id2[i]\n",
    "                \n",
    "                # Detach outputs from the graph and convert to CPU for storage\n",
    "                emb1 = output1[i].detach().cpu()\n",
    "                emb2 = output2[i].detach().cpu()\n",
    "                \n",
    "                if cid1 not in validation_embeddings:\n",
    "                    validation_embeddings[cid1] = [emb1]\n",
    "                else:\n",
    "                    validation_embeddings[cid1].append(emb1)\n",
    "                \n",
    "                if cid2 not in validation_embeddings:\n",
    "                    validation_embeddings[cid2] = [emb2]\n",
    "                else:\n",
    "                    validation_embeddings[cid2].append(emb2)\n",
    "                    \n",
    "    # print(\"Validation Embs len\", len(validation_embeddings))\n",
    "    accuracy = calculate_accuracy(model, query_dataloader, validation_embeddings, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}  Val Loss: {val_loss/len(validation_dataloader)}\")\n",
    "\n",
    "    # average_val_loss = val_loss / len(validation_dataloader)\n",
    "    # print(f'Validation - Epoch {epoch+1}/100, Loss: {average_val_loss:.4f}')\n",
    "\n",
    "    # Check if the validation loss improved\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        torch.save(model.state_dict(), 'best_acc_400.pth')\n",
    "        print(f\"Validation accuracy increased, saving model...\")\n",
    "\n",
    "    # print(f'Epoch {epoch+1}/100, Loss Val: {average_val_loss:.4f}, Accuracy Val: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), 'weights_44_classes/last_100.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load('weights_501_classes/best_acc_model.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "# Change the device to CPU\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset2/val/1793/A*ynLCQZsfnUUAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1772/A*ZHBCT7uIQ7YAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1772 1793\n",
      "dataset2/val/1803/A*KwIDTb2EoNQAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1803/A*vNCKTJi26wYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1803 1803\n",
      "dataset2/val/1810/A*9CU_SL33aGQAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1810/A*MDt2SqCph3wAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1810 1810\n",
      "dataset2/val/1779/A*UCEbTLwA0QwAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1779/A*nS0WRaWoAWMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1779 1779\n",
      "dataset2/val/1800/A*4LBtT5zFqwEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1800/A*Df-KRJqAhI8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1800 1800\n",
      "dataset2/val/1795/A*rx8nQ4vVUA4AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1795/A*I5xdT6GyKZoAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1795 1795\n",
      "dataset2/val/1811/A*8GuUT7UUgIAAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1811/A*JjnWSIqOGNAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1811 1811\n",
      "dataset2/val/1802/A*Q5bHQoXBzUcAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1802/A*AVEsT5wIaq0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1802 1802\n",
      "dataset2/val/1789/A*MvO2SJsmauQAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1789/A*cCJ5RIKf_S0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1789 1789\n",
      "dataset2/val/1807/A*_dzXQa4NeC4AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1807/A*7Vh5SZBgjiAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1807 1807\n",
      "dataset2/val/1775/A*8dSMQ6vUnmUAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1805/A*GtMyQ7JdQ-gAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1805 1775\n",
      "dataset2/val/1813/A*qiLdRILP7c8AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1813/A*8I8WQbMxkI0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1813 1813\n",
      "dataset2/val/1790/A*kRFNQK7FssgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1790/A*6j0hSpWN8QAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1790 1790\n",
      "dataset2/val/5999/A*plIEQ4aQaFEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/5999/2A23mcKTRe6iwvdle-DnoQAAACMAARAD.jpg\n",
      "5999 5999\n",
      "dataset2/val/1799/A*EAlIQK1ktQgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1799/A*SEUoS5Nl9VkAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1799 1799\n",
      "dataset2/val/1792/fiPk_1uwRfa8kmpZR47IDwAAACMAARAD.jpg ------------ dataset2/train/1792/uYlCxpMHTeWEtno0puSPFAAAACMAARAD.jpg\n",
      "1792 1792\n",
      "dataset2/val/1782/A*9j9STb2ddMYAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1772/A*9ivnRLX_20wAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1772 1782\n",
      "dataset2/val/1785/voZg6cYgSbWcFjaM0cswGQAAACMAARAD.jpg ------------ dataset2/train/1785/Ig1gLrn2T9uc1D0bn3MMjwAAACMAARAD.jpg\n",
      "1785 1785\n",
      "dataset2/val/1788/A*LJrtTIZnILEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1788/A*bbRUQpGbZrEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1788 1788\n",
      "dataset2/val/1808/A*z9b0RrLns80AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1808/A*mZoOQaJNodYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1808 1808\n",
      "dataset2/val/1796/A*TNTfRKtOhqAAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1796/A*arzWRYXY1XoAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1796 1796\n",
      "dataset2/val/1772/A*qn_WR4PpuMUAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1772/A*ZHBCT7uIQ7YAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1772 1772\n",
      "dataset2/val/1777/A*-tAtTJhSphoAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1777/A*VHShQaIIpxcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1777 1777\n",
      "dataset2/val/1771/A*LmbuQoxOuDgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1771/A*d-fSRoB7LOAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1771 1771\n",
      "dataset2/val/1781/Kpo2dxkOSMGLuUAARIF44QAAACMAARAD.jpg ------------ dataset2/train/1781/iAT5TIVJR0y77Yi92FyU4QAAACMAARAD.jpg\n",
      "1781 1781\n",
      "dataset2/val/1806/OCx_Oa6wRmy8LG6zG9ofnAAAACMAARAD.jpg ------------ dataset2/train/1806/9E3gq6i9QnqpQnMlI91rgAAAACMAARED.jpg\n",
      "1806 1806\n",
      "dataset2/val/1783/A*5CI8RZk8TJIAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1783/A*5W2zQ7IkxmAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1783 1783\n",
      "dataset2/val/1778/A*5MKkQJUztKgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1772/A*ZHBCT7uIQ7YAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1772 1778\n",
      "dataset2/val/1776/A*U9rlSZoRaYwAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1776/A*gXSwRqpe4zgAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1776 1776\n",
      "dataset2/val/1786/A*ibDoT4mK97YAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1786/A*3bmhQbHs494AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1786 1786\n",
      "dataset2/val/1787/A*QT9ZRq1yF_QAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1787/A*QT9ZRq1yF_Q4kQhIsevOqAAAAQAAAQ.jpg\n",
      "1787 1787\n",
      "dataset2/val/1797/A*X55pTr4I8EgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1797/A*NhIDTqCUOI8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1797 1797\n",
      "dataset2/val/1791/A*94FERq6L4YwAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1791/A*BjL6TbitYvQAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1791 1791\n",
      "dataset2/val/1801/A*5Ar-QbMNMQsAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1801/A*_YPdTqbfLSwAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1801 1801\n",
      "dataset2/val/1809/A*P-r1SIMSmSIAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1794/A*AIjbQZCTO4oAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1794 1809\n",
      "dataset2/val/1812/A*OlwjQp858qAAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1812/A*MBsFQbT3KmsAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1812 1812\n",
      "dataset2/val/1804/A*a0GiTpyof3EAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1804/A*8OnhQK2UlcEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1804 1804\n",
      "dataset2/val/1798/A*3rmNQo07JM8AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1798/A*6T7dTJbZ7nMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1798 1798\n",
      "dataset2/val/1794/A*t5XXT5QcC8oAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1805/A*98J-Sr75pawAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1805 1794\n",
      "dataset2/val/1773/A*vMJ-T7z_ByEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1773/A*WtsTQrt_HhgAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1773 1773\n",
      "dataset2/val/1805/A*N1noSKkPLL4AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1805/A*98J-Sr75pawAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1805 1805\n",
      "dataset2/val/1780/A*Qf-iRKyp1WgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1780/A*V_Y8RIeL4yMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1780 1780\n",
      "dataset2/val/1784/A*89CWSY3ey-oAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1784/A*tXhTR4caI_kAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1784 1784\n",
      "dataset2/val/1774/A*_ts1RLi4t6QAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1779/A*vkKzRay_Hr0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1779 1774\n",
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def get_embeddings(img_path1, img_path2, model, transform, device):\n",
    "    # Load and transform images\n",
    "    img1 = Image.open(img_path1).convert('RGB')\n",
    "    img2 = Image.open(img_path2).convert('RGB')\n",
    "    img1 = transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        output1, output2 = model(img1, img2)\n",
    "        # euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "    return output1, output2\n",
    "\n",
    "def get_all_embeddings(query_img_path, reference_img_paths, model, transform, device):\n",
    "    all_embs = {}\n",
    "    for ref_path in reference_img_paths:\n",
    "        _, ref_emb = get_embeddings(query_img_path, ref_path, model, transform, device)\n",
    "        all_embs[ref_path] = ref_emb\n",
    "\n",
    "    return all_embs\n",
    "\n",
    "def evaluate_accuracy(dataset_dir,query_imgs_dir, model, transform, device):\n",
    "    reference_images = []\n",
    "    query_imgs = []\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    # First, collect all image paths\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                reference_images.append(img_path)\n",
    "\n",
    "    for class_dir in os.listdir(query_imgs_dir):\n",
    "        class_path = os.path.join(query_imgs_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                query_imgs.append(img_path)\n",
    "\n",
    "\n",
    "    # Now evaluate each image against all collected images\n",
    "    all_embeddings = get_all_embeddings(query_imgs[0], reference_images, model, transform, device)\n",
    "    for q_img in query_imgs:\n",
    "        # most_similar_image, _ = find_most_similar(q_img, reference_images, model, transform, device)\n",
    "        q_img_embedding, _ = get_embeddings(q_img, reference_images[0], model, transform, device)\n",
    "        min_distance = float('inf')\n",
    "        most_similar_img = None\n",
    "        for ref_path, ref_emb in all_embeddings.items():\n",
    "            euclidean_distance = torch.nn.functional.pairwise_distance(q_img_embedding, ref_emb)\n",
    "            distance = euclidean_distance.item()\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                most_similar_img = ref_path\n",
    "\n",
    "        print(q_img,'------------', most_similar_img)\n",
    "        most_similar_image_class =  most_similar_img.split('/')[-2]\n",
    "        q_img_class = q_img.split('/')[-2]\n",
    "        print(most_similar_image_class, q_img_class)\n",
    "        # if os.path.dirname(most_similar_image) == os.path.dirname(q_img):\n",
    "        if most_similar_image_class == q_img_class:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    return accuracy\n",
    "# Example usage\n",
    "dataset_dir = 'dataset2/train'\n",
    "query_imgs_dir = 'dataset2/val'\n",
    "accuracy = evaluate_accuracy(dataset_dir,query_imgs_dir, model, transform, device)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog_nose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
