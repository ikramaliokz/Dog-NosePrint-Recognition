{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('pet_biometric_challenge_2022/train/train_data.csv')\n",
    "\n",
    "# # Count the number of images for each dog ID\n",
    "# image_counts = df['dog ID'].value_counts()\n",
    "\n",
    "# # Filter IDs with 5 or more images\n",
    "# ids_with_enough_images = image_counts[image_counts >= 8].index\n",
    "\n",
    "# # Create directories and copy files\n",
    "# for dog_id in ids_with_enough_images:\n",
    "#     # Create a directory for the dog ID if it doesn't exist\n",
    "#     directory_path = f'./dataset/train/{dog_id}'\n",
    "#     os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "#     # Get all images for this dog ID\n",
    "#     images_to_copy = df[df['dog ID'] == dog_id]['nose print image']\n",
    "    \n",
    "#     # Copy each image\n",
    "#     for image in images_to_copy:\n",
    "#         src_path = f'pet_biometric_challenge_2022/train/images/{image}'  # Adjust this path\n",
    "#         dst_path = f'{directory_path}/{image}'\n",
    "#         shutil.copy(src_path, dst_path)\n",
    "\n",
    "#     print(f'Copied {len(images_to_copy)} images for dog ID {dog_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir('dataset/train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class PairedDogNoseDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.dataset = ImageFolder(root=image_folder, transform=transform)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a dictionary of lists for each class\n",
    "        self.class_to_images = {}\n",
    "        for img, label in self.dataset.imgs:\n",
    "            if label not in self.class_to_images:\n",
    "                self.class_to_images[label] = []\n",
    "            self.class_to_images[label].append(img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Randomly select whether to get a positive or negative pair\n",
    "        should_get_same_class = random.randint(0, 1) == 0\n",
    "        \n",
    "        first_image, label1 = self.dataset.imgs[index]\n",
    "        if should_get_same_class:\n",
    "            second_image = random.choice(self.class_to_images[label1])\n",
    "            label = 1\n",
    "        else:\n",
    "            different_class = random.choice(list(set(self.dataset.class_to_idx.values()) - {label1}))\n",
    "            second_image = random.choice(self.class_to_images[different_class])\n",
    "            label = 0\n",
    "\n",
    "        img1 = Image.open(first_image)\n",
    "        img2 = Image.open(second_image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transformations - you can add more augmentations as necessary\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),          # Convert images to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "paired_train_dataset = PairedDogNoseDataset('dataset/train', transform=transform)\n",
    "paired_train_dataloader = DataLoader(paired_train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, s=30.0, m=0.50):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, cosine, labels):\n",
    "        # Add margin\n",
    "        phi = cosine - self.m\n",
    "        # Apply the softmax on the adjusted scores\n",
    "        one_hot = torch.zeros(cosine.size(), device=cosine.device)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # only adjust the angles for correct class\n",
    "        output *= self.s\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(output, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        # Load a pre-trained ResNet-152 and remove the last GAP and FC\n",
    "        base_model = resnet152(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "        # Additional blocks to reduce channel dimensions\n",
    "        self.reduce_channels = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, kernel_size=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.reduce_channels(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        ca = self.channel_attention(x) * x\n",
    "\n",
    "        # Spatial attention\n",
    "        sa = self.spatial_attention(ca) * ca\n",
    "\n",
    "        return sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = ResNetBackbone()\n",
    "        self.attention = AttentionModule()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, 1024)  # Output 1024-dimensional embedding\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.backbone(x1)\n",
    "        out1 = self.attention(out1)\n",
    "        out1 = self.pooling(out1)\n",
    "        out1 = out1.view(out1.size(0), -1)\n",
    "        out1 = self.fc(out1)\n",
    "\n",
    "        out2 = self.backbone(x2)\n",
    "        out2 = self.attention(out2)\n",
    "        out2 = self.pooling(out2)\n",
    "        out2 = out2.view(out2.size(0), -1)\n",
    "        out2 = self.fc(out2)\n",
    "\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, targets, threshold=0.5):\n",
    "    \"\"\" Computes the accuracy based on a threshold.\n",
    "        Predictions below the threshold are considered 'positive' or 'same class'.\n",
    "    \"\"\"\n",
    "    predictions = predictions < threshold\n",
    "    correct = (predictions == targets).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athenaai/anaconda3/envs/dog_nose/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/athenaai/anaconda3/envs/dog_nose/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.4477, Accuracy: 0.5139\n",
      "Epoch 2/50, Loss: 0.3300, Accuracy: 0.6611\n",
      "Epoch 3/50, Loss: 0.3931, Accuracy: 0.6028\n",
      "Epoch 4/50, Loss: 0.1165, Accuracy: 0.5778\n",
      "Epoch 5/50, Loss: 0.3846, Accuracy: 0.6167\n",
      "Epoch 6/50, Loss: 0.3168, Accuracy: 0.6056\n",
      "Epoch 7/50, Loss: 0.7623, Accuracy: 0.6500\n",
      "Epoch 8/50, Loss: 0.3608, Accuracy: 0.6028\n",
      "Epoch 9/50, Loss: 0.3390, Accuracy: 0.5778\n",
      "Epoch 10/50, Loss: 0.1655, Accuracy: 0.6222\n",
      "Epoch 11/50, Loss: 0.3990, Accuracy: 0.6056\n",
      "Epoch 12/50, Loss: 0.3650, Accuracy: 0.5806\n",
      "Epoch 13/50, Loss: 0.3736, Accuracy: 0.6500\n",
      "Epoch 14/50, Loss: 0.3609, Accuracy: 0.6361\n",
      "Epoch 15/50, Loss: 0.3827, Accuracy: 0.5944\n",
      "Epoch 16/50, Loss: 0.4683, Accuracy: 0.5583\n",
      "Epoch 17/50, Loss: 0.3165, Accuracy: 0.6333\n",
      "Epoch 18/50, Loss: 0.3507, Accuracy: 0.5833\n",
      "Epoch 19/50, Loss: 0.2905, Accuracy: 0.6222\n",
      "Epoch 20/50, Loss: 0.3474, Accuracy: 0.5667\n",
      "Epoch 21/50, Loss: 0.3718, Accuracy: 0.6333\n",
      "Epoch 22/50, Loss: 0.3976, Accuracy: 0.6056\n",
      "Epoch 23/50, Loss: 0.3459, Accuracy: 0.6111\n",
      "Epoch 24/50, Loss: 0.1206, Accuracy: 0.5722\n",
      "Epoch 25/50, Loss: 0.1669, Accuracy: 0.5444\n",
      "Epoch 26/50, Loss: 0.1845, Accuracy: 0.5444\n",
      "Epoch 27/50, Loss: 0.3591, Accuracy: 0.5611\n",
      "Epoch 28/50, Loss: 0.3581, Accuracy: 0.6111\n",
      "Epoch 29/50, Loss: 0.3831, Accuracy: 0.5861\n",
      "Epoch 30/50, Loss: 0.3428, Accuracy: 0.5361\n",
      "Epoch 31/50, Loss: 0.3966, Accuracy: 0.5889\n",
      "Epoch 32/50, Loss: 0.3642, Accuracy: 0.5389\n",
      "Epoch 33/50, Loss: 0.3664, Accuracy: 0.5750\n",
      "Epoch 34/50, Loss: 0.3678, Accuracy: 0.5361\n",
      "Epoch 35/50, Loss: 0.3670, Accuracy: 0.5778\n",
      "Epoch 36/50, Loss: 0.2768, Accuracy: 0.5611\n",
      "Epoch 37/50, Loss: 0.0564, Accuracy: 0.5889\n",
      "Epoch 38/50, Loss: 0.1394, Accuracy: 0.4722\n",
      "Epoch 39/50, Loss: 0.1194, Accuracy: 0.5500\n",
      "Epoch 40/50, Loss: 0.3588, Accuracy: 0.5861\n",
      "Epoch 41/50, Loss: 0.4001, Accuracy: 0.5639\n",
      "Epoch 42/50, Loss: 0.3637, Accuracy: 0.5583\n",
      "Epoch 43/50, Loss: 0.6469, Accuracy: 0.5778\n",
      "Epoch 44/50, Loss: 0.6749, Accuracy: 0.5667\n",
      "Epoch 45/50, Loss: 0.1607, Accuracy: 0.5611\n",
      "Epoch 46/50, Loss: 0.3720, Accuracy: 0.5472\n",
      "Epoch 47/50, Loss: 0.5140, Accuracy: 0.5778\n",
      "Epoch 48/50, Loss: 0.2842, Accuracy: 0.5472\n",
      "Epoch 49/50, Loss: 0.3605, Accuracy: 0.5722\n",
      "Epoch 50/50, Loss: 0.3729, Accuracy: 0.5861\n"
     ]
    }
   ],
   "source": [
    "# Assuming model, optimizers, and loss functions are already defined\n",
    "model = SiameseNetwork().to(device)\n",
    "optimizer_contrastive = optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_arcface = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "criterion_contrastive = ContrastiveLoss(margin=2.0)\n",
    "criterion_arcface = ArcFaceLoss(s=30, m=0.5)\n",
    "\n",
    "# Scheduler for learning rate decay of the ArcFace optimizer\n",
    "scheduler_arcface = optim.lr_scheduler.LinearLR(optimizer_arcface, start_factor=1.0, end_factor=0, total_iters=200)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):  # Total epochs set as 200\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for img1, img2, labels in paired_train_dataloader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "        output1, output2 = model(img1, img2)\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        # Compute Contrastive Loss\n",
    "        loss_contrastive = criterion_contrastive(output1, output2, labels)\n",
    "        \n",
    "        # Calculate the mean embedding for ArcFace Loss (simplified version)\n",
    "        embeddings = (output1 + output2) / 2\n",
    "        loss_arcface = criterion_arcface(embeddings, labels)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = loss_contrastive + loss_arcface\n",
    "        # print(f'Epoch {epoch+1}, Loss: {total_loss.item():.4f}')\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = compute_accuracy(euclidean_distance, labels, threshold=0.5)\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer_contrastive.zero_grad()\n",
    "        optimizer_arcface.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer_contrastive.step()\n",
    "        optimizer_arcface.step()\n",
    "    \n",
    "    scheduler_arcface.step()  # Update the learning rate\n",
    "    \n",
    "    average_accuracy = total_accuracy / len(paired_train_dataloader)\n",
    "    average_loss = total_loss / len(paired_train_dataloader)\n",
    "    print(f'Epoch {epoch+1}/50, Loss: {average_loss:.4f}, Accuracy: {average_accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), 'weight.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = SiameseNetwork().cuda()\n",
    "model.load_state_dict(torch.load('weight.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "# Change the device to CPU\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_similarity(img_path1, img_path2):\n",
    "    # Load and transform images\n",
    "    img1 = Image.open(img_path1).convert('RGB')\n",
    "    img2 = Image.open(img_path2).convert('RGB')\n",
    "    img1 = transform(img1).unsqueeze(0).to(device) # Add batch dimension and send to GPU\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)  # Add batch dimension and send to GPU\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        output1, output2 = model(img1, img2)\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "\n",
    "    return euclidean_distance.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between the images is: 0.7601126432418823\n"
     ]
    }
   ],
   "source": [
    "# Paths to the images you want to compare\n",
    "image_path1 = 'dataset/train/1771/A*d-fSRoB7LOAAAAAAAAAAAAAAAQAAAQ.jpg'\n",
    "image_path2 = 'dataset/train/1775/A*8dSMQ6vUnmUAAAAAAAAAAAAAAQAAAQ.jpg'\n",
    "\n",
    "# Perform inference\n",
    "distance = infer_similarity(image_path1, image_path2)\n",
    "print(f'The Euclidean distance between the images is: {distance}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar image: dataset/train/1781/0Tolu7cpRQurgl7b87DAOwAAACMAARAD.jpg, Distance: 0.08487781137228012\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(query_img_path, reference_img_paths):\n",
    "    min_distance = float('inf')\n",
    "    most_similar_img = None\n",
    "\n",
    "    for ref_path in reference_img_paths:\n",
    "        distance = infer_similarity(query_img_path, ref_path)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            most_similar_img = ref_path\n",
    "\n",
    "    return most_similar_img, min_distance\n",
    "\n",
    "# List of reference image paths\n",
    "reference_images = ['dataset/train/1781/0Tolu7cpRQurgl7b87DAOwAAACMAARAD.jpg','dataset/train/1790/A*Kf4tQrPJVrgAAAAAAAAAAAAAAQAAAQ.jpg', 'dataset/train/1771/A*jP8SQ4cdHVDm2wJBthDn0AAAAQAAAQ.jpg', 'dataset/train/1775/A*cSJSR4QTq74AAAAAAAAAAAAAAQAAAQ.jpg', 'dataset/train/1780/A*83ofSqE-8DMAAAAAAAAAAAAAAQAAAQ.jpg']\n",
    "most_similar_image, similarity_score = find_most_similar('dataset/train/1781/miLuamg6Tx-rWnXEs7jOuwAAACMAARAD.jpg', reference_images)\n",
    "print(f'Most similar image: {most_similar_image}, Distance: {similarity_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar:  dataset/testing/1775/A*8dSMQ6vUnmUAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*8dSMQ6vUnmUAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*Vnv4RqS3jBwAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*Vnv4RqS3jBwAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*LxrVTamXKDcAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*LxrVTamXKDcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*1EHJS5BugqUAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*1EHJS5BugqUAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*4ByBQ6x73bcAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*4ByBQ6x73bcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*cSJSR4QTq74AAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*cSJSR4QTq74AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*mIswT5Z4vjAAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*mIswT5Z4vjAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1775/A*8sRFTYtX8IUAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1775/A*8sRFTYtX8IUAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*qn_WR4PpuMUAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*qn_WR4PpuMUAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*ZHBCT7uIQ7YAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*ZHBCT7uIQ7YAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*I7EGS69kpoAAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*I7EGS69kpoAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*__WEQ73cq1sAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*__WEQ73cq1sAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*YAbERaY0Us4AAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*YAbERaY0Us4AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*ksEYTIF7hHkAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*ksEYTIF7hHkAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*FaR_RZstqUEAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*FaR_RZstqUEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1772/A*9ivnRLX_20wAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1772/A*9ivnRLX_20wAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*LmbuQoxOuDgAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*LmbuQoxOuDgAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*jP8SQ4cdHVDm2wJBthDn0AAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*jP8SQ4cdHVDm2wJBthDn0AAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*jP8SQ4cdHVDm2wJBthDn0AAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*jP8SQ4cdHVDHuaVGsWP_IwAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*jP8SQ4cdHVDm2wJBthDn0AAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*jP8SQ4cdHVAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*EHgRTLiRPpcAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*EHgRTLiRPpcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*d-fSRoB7LOAAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*d-fSRoB7LOAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*uKATSqHOo9AAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*uKATSqHOo9AAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1771/A*GMS_SJtg75QAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1771/A*GMS_SJtg75QAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*vMJ-T7z_ByEAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*vMJ-T7z_ByEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*WtsTQrt_HhgAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*WtsTQrt_HhgAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*HtbgRrGx8jIAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*HtbgRrGx8jIAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*XSTqRI4hcmcAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*XSTqRI4hcmcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*EffrQb4sZEcAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*EffrQb4sZEcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*jLojSo6dQPIAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*jLojSo6dQPIAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*BTHSSoW0nMAAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*BTHSSoW0nMAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1773/A*j8dsSIfQ7DYAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1773/A*j8dsSIfQ7DYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*_ts1RLi4t6QAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*_ts1RLi4t6QAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*R219QYYP14AAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*R219QYYP14AAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*QlABTZmAYIYAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*QlABTZmAYIYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*2vGQR6TKWsMAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*2vGQR6TKWsMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*6A8LQ7KTql8AAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*6A8LQ7KTql8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*S8piQ4XCPAoAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*S8piQ4XCPAoAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*a24pSZVP0jIAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*a24pSZVP0jIAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Most Similar:  dataset/testing/1774/A*l6tgR4j42wEAAAAAAAAAAAAAAQAAAQ.jpg     ref img:  dataset/testing/1774/A*l6tgR4j42wEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def infer_similarity(img_path1, img_path2, model, transform, device):\n",
    "    # Load and transform images\n",
    "    img1 = Image.open(img_path1).convert('RGB')\n",
    "    img2 = Image.open(img_path2).convert('RGB')\n",
    "    img1 = transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        output1, output2 = model(img1, img2)\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "    return euclidean_distance.item()\n",
    "\n",
    "def find_most_similar(query_img_path, reference_img_paths, model, transform, device):\n",
    "    min_distance = float('inf')\n",
    "    most_similar_img = None\n",
    "\n",
    "    for ref_path in reference_img_paths:\n",
    "        distance = infer_similarity(query_img_path, ref_path, model, transform, device)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            most_similar_img = ref_path\n",
    "\n",
    "    return most_similar_img, min_distance\n",
    "\n",
    "def evaluate_accuracy(dataset_dir, model, transform, device):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    reference_images = []\n",
    "\n",
    "    # First, collect all image paths\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                reference_images.append(img_path)\n",
    "\n",
    "    # Now evaluate each image against all collected images\n",
    "    for ref_img in reference_images:\n",
    "        most_similar_image, _ = find_most_similar(ref_img, reference_images, model, transform, device)\n",
    "        print(\"Most Similar: \",most_similar_image, \"    ref img: \", ref_img)\n",
    "        if os.path.dirname(most_similar_image) == os.path.dirname(ref_img):\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "dataset_dir = 'dataset/testing'\n",
    "accuracy = evaluate_accuracy(dataset_dir, model, transform, device)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset2/val/1793/A*ynLCQZsfnUUAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1777/A*3HkHR4krq0oAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1777 1793\n",
      "dataset2/val/1803/A*KwIDTb2EoNQAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1784/A*pvbPR5VSOlcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1784 1803\n",
      "dataset2/val/1810/A*9CU_SL33aGQAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1810/A*MDt2SqCph3wAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1810 1810\n",
      "dataset2/val/1779/A*UCEbTLwA0QwAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1807/A*AywAQq_O7fAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1807 1779\n",
      "dataset2/val/1800/A*4LBtT5zFqwEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1778/A*pyU2TZ8yOTsAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1778 1800\n",
      "dataset2/val/1795/A*rx8nQ4vVUA4AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1774/A*6A8LQ7KTql8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1774 1795\n",
      "dataset2/val/1811/A*8GuUT7UUgIAAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1783/A*W0oZR4jl1fYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1783 1811\n",
      "dataset2/val/1802/A*Q5bHQoXBzUcAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1784/A*_RA_QIYit_sAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1784 1802\n",
      "dataset2/val/1789/A*MvO2SJsmauQAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1789/A*cCJ5RIKf_S0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1789 1789\n",
      "dataset2/val/1807/A*_dzXQa4NeC4AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1791/A*TwaHTamkfzYAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1791 1807\n",
      "dataset2/val/1775/A*8dSMQ6vUnmUAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1809/A*V5bURInqcm8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1809 1775\n",
      "dataset2/val/1813/A*qiLdRILP7c8AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1792/krGBsWt7QZq2UlEFwetRGQAAACMAARAD.jpg\n",
      "1792 1813\n",
      "dataset2/val/1790/A*kRFNQK7FssgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1790/A*6j0hSpWN8QAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1790 1790\n",
      "dataset2/val/5999/A*plIEQ4aQaFEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/5999/2A23mcKTRe6iwvdle-DnoQAAACMAARAD.jpg\n",
      "5999 5999\n",
      "dataset2/val/1799/A*EAlIQK1ktQgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1801/A*2sNJT55svD4AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1801 1799\n",
      "dataset2/val/1792/fiPk_1uwRfa8kmpZR47IDwAAACMAARAD.jpg ------------ dataset2/train/1813/A*8KjsQp7HMjMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1813 1792\n",
      "dataset2/val/1782/A*9j9STb2ddMYAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1775/A*4ByBQ6x73bcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1775 1782\n",
      "dataset2/val/1785/voZg6cYgSbWcFjaM0cswGQAAACMAARAD.jpg ------------ dataset2/train/1804/A*8OnhQK2UlcEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1804 1785\n",
      "dataset2/val/1788/A*LJrtTIZnILEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1788/A*i3LGRJJcMxIAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1788 1788\n",
      "dataset2/val/1808/A*z9b0RrLns80AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1808/A*vr4uQ7tchxUAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1808 1808\n",
      "dataset2/val/1796/A*TNTfRKtOhqAAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1796/A*p5f4R4q47foAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1796 1796\n",
      "dataset2/val/1772/A*qn_WR4PpuMUAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1789/A*cCJ5RIKf_S0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1789 1772\n",
      "dataset2/val/1777/A*-tAtTJhSphoAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1773/A*BTHSSoW0nMAAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1773 1777\n",
      "dataset2/val/1771/A*LmbuQoxOuDgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1809/A*V5bURInqcm8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1809 1771\n",
      "dataset2/val/1781/Kpo2dxkOSMGLuUAARIF44QAAACMAARAD.jpg ------------ dataset2/train/1781/d1sSMlRGTjqe1gx-bbo6swAAACMAARAD.jpg\n",
      "1781 1781\n",
      "dataset2/val/1806/OCx_Oa6wRmy8LG6zG9ofnAAAACMAARAD.jpg ------------ dataset2/train/1806/9E3gq6i9QnqpQnMlI91rgAAAACMAARED.jpg\n",
      "1806 1806\n",
      "dataset2/val/1783/A*5CI8RZk8TJIAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1794/A*sYJBQ4GrqjoAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1794 1783\n",
      "dataset2/val/1778/A*5MKkQJUztKgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1788/A*Cx3bRbUjOn0AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1788 1778\n",
      "dataset2/val/1776/A*U9rlSZoRaYwAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1776/A*RcR6QYzsgeMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1776 1776\n",
      "dataset2/val/1786/A*ibDoT4mK97YAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1809/A*l-hQRbyLRx8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1809 1786\n",
      "dataset2/val/1787/A*QT9ZRq1yF_QAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1787/A*QT9ZRq1yF_Q4kQhIsevOqAAAAQAAAQ.jpg\n",
      "1787 1787\n",
      "dataset2/val/1797/A*X55pTr4I8EgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1813/A*CfqeRZKNdTEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1813 1797\n",
      "dataset2/val/1791/A*94FERq6L4YwAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1791/A*XtdjRIwS2V8AAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1791 1791\n",
      "dataset2/val/1801/A*5Ar-QbMNMQsAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1779/A*BEE6Qosja1sAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1779 1801\n",
      "dataset2/val/1809/A*P-r1SIMSmSIAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1806/a0NOVEVNTsaiS5nIbfB8mAAAACMAARAD.jpg\n",
      "1806 1809\n",
      "dataset2/val/1812/A*OlwjQp858qAAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1803/A*S2qYR5YRuWQAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1803 1812\n",
      "dataset2/val/1804/A*a0GiTpyof3EAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1804/A*8OnhQK2UlcEAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1804 1804\n",
      "dataset2/val/1798/A*3rmNQo07JM8AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1798/A*6T7dTJbZ7nMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1798 1798\n",
      "dataset2/val/1794/A*t5XXT5QcC8oAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1794/A*sYJBQ4GrqjoAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1794 1794\n",
      "dataset2/val/1773/A*vMJ-T7z_ByEAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1783/A*mvu3T6PdcFkAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1783 1773\n",
      "dataset2/val/1805/A*N1noSKkPLL4AAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1795/A*niyFTaQpECkAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1795 1805\n",
      "dataset2/val/1780/A*Qf-iRKyp1WgAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1773/A*XSTqRI4hcmcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1773 1780\n",
      "dataset2/val/1784/A*89CWSY3ey-oAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1773/A*XSTqRI4hcmcAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1773 1784\n",
      "dataset2/val/1774/A*_ts1RLi4t6QAAAAAAAAAAAAAAQAAAQ.jpg ------------ dataset2/train/1780/A*V_Y8RIeL4yMAAAAAAAAAAAAAAQAAAQ.jpg\n",
      "1780 1774\n",
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def get_embeddings(img_path1, img_path2, model, transform, device):\n",
    "    # Load and transform images\n",
    "    img1 = Image.open(img_path1).convert('RGB')\n",
    "    img2 = Image.open(img_path2).convert('RGB')\n",
    "    img1 = transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        output1, output2 = model(img1, img2)\n",
    "        # euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "    return output1, output2\n",
    "\n",
    "def get_all_embeddings(query_img_path, reference_img_paths, model, transform, device):\n",
    "    all_embs = {}\n",
    "    for ref_path in reference_img_paths:\n",
    "        _, ref_emb = get_embeddings(query_img_path, ref_path, model, transform, device)\n",
    "        all_embs[ref_path] = ref_emb\n",
    "\n",
    "    return all_embs\n",
    "\n",
    "def evaluate_accuracy(dataset_dir,query_imgs_dir, model, transform, device):\n",
    "    reference_images = []\n",
    "    query_imgs = []\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    # First, collect all image paths\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                reference_images.append(img_path)\n",
    "\n",
    "    for class_dir in os.listdir(query_imgs_dir):\n",
    "        class_path = os.path.join(query_imgs_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                query_imgs.append(img_path)\n",
    "\n",
    "\n",
    "    # Now evaluate each image against all collected images\n",
    "    all_embeddings = get_all_embeddings(query_imgs[0], reference_images, model, transform, device)\n",
    "    for q_img in query_imgs:\n",
    "        # most_similar_image, _ = find_most_similar(q_img, reference_images, model, transform, device)\n",
    "        q_img_embedding, _ = get_embeddings(q_img, reference_images[0], model, transform, device)\n",
    "        min_distance = float('inf')\n",
    "        most_similar_img = None\n",
    "        for ref_path, ref_emb in all_embeddings.items():\n",
    "            euclidean_distance = torch.nn.functional.pairwise_distance(q_img_embedding, ref_emb)\n",
    "            distance = euclidean_distance.item()\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                most_similar_img = ref_path\n",
    "\n",
    "        print(q_img,'------------', most_similar_img)\n",
    "        most_similar_image_class =  most_similar_img.split('/')[-2]\n",
    "        q_img_class = q_img.split('/')[-2]\n",
    "        print(most_similar_image_class, q_img_class)\n",
    "        # if os.path.dirname(most_similar_image) == os.path.dirname(q_img):\n",
    "        if most_similar_image_class == q_img_class:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    return accuracy\n",
    "# Example usage\n",
    "dataset_dir = 'dataset2/train'\n",
    "query_imgs_dir = 'dataset2/val'\n",
    "accuracy = evaluate_accuracy(dataset_dir,query_imgs_dir, model, transform, device)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog_nose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
