{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('pet_biometric_challenge_2022/train/train_data.csv')\n",
    "\n",
    "# # Count the number of images for each dog ID\n",
    "# image_counts = df['dog ID'].value_counts()\n",
    "\n",
    "# # Filter IDs with 5 or more images\n",
    "# ids_with_enough_images = image_counts[image_counts >= 8].index\n",
    "\n",
    "# # Create directories and copy files\n",
    "# for dog_id in ids_with_enough_images:\n",
    "#     # Create a directory for the dog ID if it doesn't exist\n",
    "#     directory_path = f'./dataset/train/{dog_id}'\n",
    "#     os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "#     # Get all images for this dog ID\n",
    "#     images_to_copy = df[df['dog ID'] == dog_id]['nose print image']\n",
    "    \n",
    "#     # Copy each image\n",
    "#     for image in images_to_copy:\n",
    "#         src_path = f'pet_biometric_challenge_2022/train/images/{image}'  # Adjust this path\n",
    "#         dst_path = f'{directory_path}/{image}'\n",
    "#         shutil.copy(src_path, dst_path)\n",
    "\n",
    "#     print(f'Copied {len(images_to_copy)} images for dog ID {dog_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir('dataset/train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class PairedDogNoseDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.dataset = ImageFolder(root=image_folder, transform=transform)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a dictionary of lists for each class\n",
    "        self.class_to_images = {}\n",
    "        for img, label in self.dataset.imgs:\n",
    "            if label not in self.class_to_images:\n",
    "                self.class_to_images[label] = []\n",
    "            self.class_to_images[label].append(img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Randomly select whether to get a positive or negative pair\n",
    "        should_get_same_class = random.randint(0, 1) == 0\n",
    "        \n",
    "        first_image, label1 = self.dataset.imgs[index]\n",
    "        if should_get_same_class:\n",
    "            second_image = random.choice(self.class_to_images[label1])\n",
    "            label = 1\n",
    "        else:\n",
    "            different_class = random.choice(list(set(self.dataset.class_to_idx.values()) - {label1}))\n",
    "            second_image = random.choice(self.class_to_images[different_class])\n",
    "            label = 0\n",
    "\n",
    "        img1 = Image.open(first_image)\n",
    "        img2 = Image.open(second_image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transformations - you can add more augmentations as necessary\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),          # Convert images to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "paired_train_dataset = PairedDogNoseDataset('dataset/train', transform=transform)\n",
    "paired_train_dataloader = DataLoader(paired_train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, s=30.0, m=0.50):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, cosine, labels):\n",
    "        # Add margin\n",
    "        phi = cosine - self.m\n",
    "        # Apply the softmax on the adjusted scores\n",
    "        one_hot = torch.zeros(cosine.size(), device=cosine.device)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # only adjust the angles for correct class\n",
    "        output *= self.s\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(output, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        # Load a pre-trained ResNet-152 and remove the last GAP and FC\n",
    "        base_model = resnet152(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "        # Additional blocks to reduce channel dimensions\n",
    "        self.reduce_channels = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, kernel_size=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.reduce_channels(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        ca = self.channel_attention(x) * x\n",
    "\n",
    "        # Spatial attention\n",
    "        sa = self.spatial_attention(ca) * ca\n",
    "\n",
    "        return sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = ResNetBackbone()\n",
    "        self.attention = AttentionModule()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, 1024)  # Output 1024-dimensional embedding\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.backbone(x1)\n",
    "        out1 = self.attention(out1)\n",
    "        out1 = self.pooling(out1)\n",
    "        out1 = out1.view(out1.size(0), -1)\n",
    "        out1 = self.fc(out1)\n",
    "\n",
    "        out2 = self.backbone(x2)\n",
    "        out2 = self.attention(out2)\n",
    "        out2 = self.pooling(out2)\n",
    "        out2 = out2.view(out2.size(0), -1)\n",
    "        out2 = self.fc(out2)\n",
    "\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athenaai/anaconda3/envs/dog_nose/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/athenaai/anaconda3/envs/dog_nose/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 26.1961\n",
      "Epoch 1, Loss: 25.2341\n",
      "Epoch 1, Loss: 21.9405\n",
      "Epoch 1, Loss: 19.3104\n",
      "Epoch 1, Loss: 20.3678\n",
      "Epoch 1, Loss: 18.0179\n",
      "Epoch 1, Loss: 16.7515\n",
      "Epoch 1, Loss: 12.9856\n",
      "Epoch 1, Loss: 14.7783\n",
      "Epoch 1, Loss: 23.6225\n",
      "Epoch 1, Loss: 16.8969\n",
      "Epoch 1, Loss: 17.0347\n",
      "Epoch 1, Loss: 20.6166\n",
      "Epoch 1, Loss: 17.8448\n",
      "Epoch 1, Loss: 14.6770\n",
      "Epoch 1, Loss: 16.6490\n",
      "Epoch 1, Loss: 16.5824\n",
      "Epoch 1, Loss: 15.5156\n",
      "Epoch 1, Loss: 11.3931\n",
      "Epoch 1, Loss: 10.7938\n",
      "Epoch 1, Loss: 19.6931\n",
      "Epoch 1, Loss: 16.4525\n",
      "Epoch 1, Loss: 14.0477\n",
      "Epoch 1, Loss: 14.0073\n",
      "Epoch 1, Loss: 16.5781\n",
      "Epoch 1, Loss: 21.7767\n",
      "Epoch 1, Loss: 23.6572\n",
      "Epoch 1, Loss: 19.4430\n",
      "Epoch 1, Loss: 17.2542\n",
      "Epoch 1, Loss: 22.2114\n",
      "Epoch 1, Loss: 15.6215\n",
      "Epoch 1, Loss: 18.2482\n",
      "Epoch 1, Loss: 15.4398\n",
      "Epoch 1, Loss: 15.3676\n",
      "Epoch 1, Loss: 16.5256\n",
      "Epoch 1, Loss: 18.0599\n",
      "Epoch 1, Loss: 17.0484\n",
      "Epoch 1, Loss: 16.5431\n",
      "Epoch 1, Loss: 17.5243\n",
      "Epoch 1, Loss: 15.0646\n",
      "Epoch 1, Loss: 17.0666\n",
      "Epoch 1, Loss: 16.3739\n",
      "Epoch 1, Loss: 12.1146\n",
      "Epoch 1, Loss: 19.5798\n",
      "Epoch 1, Loss: 12.1934\n",
      "Epoch 1/200, Loss: 12.1934\n",
      "Epoch 2, Loss: 17.2918\n",
      "Epoch 2, Loss: 17.2274\n",
      "Epoch 2, Loss: 15.0196\n",
      "Epoch 2, Loss: 15.9639\n",
      "Epoch 2, Loss: 9.3940\n",
      "Epoch 2, Loss: 17.3300\n",
      "Epoch 2, Loss: 13.1874\n",
      "Epoch 2, Loss: 18.4645\n",
      "Epoch 2, Loss: 16.6630\n",
      "Epoch 2, Loss: 13.0038\n",
      "Epoch 2, Loss: 11.4826\n",
      "Epoch 2, Loss: 16.6432\n",
      "Epoch 2, Loss: 16.2269\n",
      "Epoch 2, Loss: 13.4629\n",
      "Epoch 2, Loss: 12.8771\n",
      "Epoch 2, Loss: 6.6090\n",
      "Epoch 2, Loss: 17.1255\n",
      "Epoch 2, Loss: 5.8069\n",
      "Epoch 2, Loss: 13.6045\n",
      "Epoch 2, Loss: 13.7757\n",
      "Epoch 2, Loss: 20.9783\n",
      "Epoch 2, Loss: 16.2037\n",
      "Epoch 2, Loss: 20.6262\n",
      "Epoch 2, Loss: 19.1761\n",
      "Epoch 2, Loss: 17.7534\n",
      "Epoch 2, Loss: 19.2892\n",
      "Epoch 2, Loss: 16.8918\n",
      "Epoch 2, Loss: 17.8332\n",
      "Epoch 2, Loss: 16.7626\n",
      "Epoch 2, Loss: 16.1789\n",
      "Epoch 2, Loss: 17.0756\n",
      "Epoch 2, Loss: 15.8314\n",
      "Epoch 2, Loss: 15.0793\n",
      "Epoch 2, Loss: 12.8816\n",
      "Epoch 2, Loss: 16.3624\n",
      "Epoch 2, Loss: 13.9055\n",
      "Epoch 2, Loss: 14.7935\n",
      "Epoch 2, Loss: 17.6424\n",
      "Epoch 2, Loss: 14.8434\n",
      "Epoch 2, Loss: 24.5786\n",
      "Epoch 2, Loss: 14.9391\n",
      "Epoch 2, Loss: 21.3798\n",
      "Epoch 2, Loss: 19.5173\n",
      "Epoch 2, Loss: 14.0002\n",
      "Epoch 2, Loss: 22.9277\n",
      "Epoch 2/200, Loss: 22.9277\n",
      "Epoch 3, Loss: 14.5948\n",
      "Epoch 3, Loss: 19.7004\n",
      "Epoch 3, Loss: 18.7885\n",
      "Epoch 3, Loss: 15.4719\n",
      "Epoch 3, Loss: 19.1287\n",
      "Epoch 3, Loss: 22.0863\n",
      "Epoch 3, Loss: 12.5377\n",
      "Epoch 3, Loss: 19.6971\n",
      "Epoch 3, Loss: 17.1347\n",
      "Epoch 3, Loss: 13.5624\n",
      "Epoch 3, Loss: 18.8653\n",
      "Epoch 3, Loss: 22.2334\n",
      "Epoch 3, Loss: 18.3955\n",
      "Epoch 3, Loss: 15.9802\n",
      "Epoch 3, Loss: 15.0102\n",
      "Epoch 3, Loss: 17.7520\n",
      "Epoch 3, Loss: 16.9562\n",
      "Epoch 3, Loss: 17.8891\n",
      "Epoch 3, Loss: 15.8364\n",
      "Epoch 3, Loss: 18.2971\n",
      "Epoch 3, Loss: 16.2944\n",
      "Epoch 3, Loss: 16.3115\n",
      "Epoch 3, Loss: 14.9208\n",
      "Epoch 3, Loss: 16.8888\n",
      "Epoch 3, Loss: 17.6334\n",
      "Epoch 3, Loss: 17.8855\n",
      "Epoch 3, Loss: 15.2642\n",
      "Epoch 3, Loss: 17.0752\n",
      "Epoch 3, Loss: 16.5486\n",
      "Epoch 3, Loss: 17.7631\n",
      "Epoch 3, Loss: 16.9417\n",
      "Epoch 3, Loss: 14.0249\n",
      "Epoch 3, Loss: 15.3875\n",
      "Epoch 3, Loss: 16.0796\n",
      "Epoch 3, Loss: 19.8611\n",
      "Epoch 3, Loss: 16.1819\n",
      "Epoch 3, Loss: 15.7076\n",
      "Epoch 3, Loss: 16.6538\n",
      "Epoch 3, Loss: 14.9966\n",
      "Epoch 3, Loss: 18.2604\n",
      "Epoch 3, Loss: 16.9366\n",
      "Epoch 3, Loss: 17.3303\n",
      "Epoch 3, Loss: 17.5756\n",
      "Epoch 3, Loss: 14.4251\n",
      "Epoch 3, Loss: 16.4650\n",
      "Epoch 3/200, Loss: 16.4650\n",
      "Epoch 4, Loss: 15.9914\n",
      "Epoch 4, Loss: 15.7999\n",
      "Epoch 4, Loss: 12.9770\n",
      "Epoch 4, Loss: 15.7671\n",
      "Epoch 4, Loss: 17.0526\n",
      "Epoch 4, Loss: 20.8038\n",
      "Epoch 4, Loss: 18.1968\n",
      "Epoch 4, Loss: 16.3806\n",
      "Epoch 4, Loss: 14.4200\n",
      "Epoch 4, Loss: 18.2602\n",
      "Epoch 4, Loss: 17.0709\n",
      "Epoch 4, Loss: 14.4943\n",
      "Epoch 4, Loss: 15.8151\n",
      "Epoch 4, Loss: 15.4027\n",
      "Epoch 4, Loss: 13.5562\n",
      "Epoch 4, Loss: 16.2894\n",
      "Epoch 4, Loss: 14.1143\n",
      "Epoch 4, Loss: 16.1259\n",
      "Epoch 4, Loss: 18.2154\n",
      "Epoch 4, Loss: 14.5676\n",
      "Epoch 4, Loss: 16.7429\n",
      "Epoch 4, Loss: 18.1944\n",
      "Epoch 4, Loss: 14.2312\n",
      "Epoch 4, Loss: 18.0061\n",
      "Epoch 4, Loss: 18.8319\n",
      "Epoch 4, Loss: 18.8596\n",
      "Epoch 4, Loss: 19.3140\n",
      "Epoch 4, Loss: 15.2421\n",
      "Epoch 4, Loss: 16.3615\n",
      "Epoch 4, Loss: 18.5756\n",
      "Epoch 4, Loss: 16.1516\n",
      "Epoch 4, Loss: 16.2413\n",
      "Epoch 4, Loss: 15.6286\n",
      "Epoch 4, Loss: 16.2623\n",
      "Epoch 4, Loss: 16.5467\n",
      "Epoch 4, Loss: 15.0208\n",
      "Epoch 4, Loss: 17.4495\n",
      "Epoch 4, Loss: 15.6292\n",
      "Epoch 4, Loss: 16.6702\n",
      "Epoch 4, Loss: 19.8449\n",
      "Epoch 4, Loss: 14.5630\n",
      "Epoch 4, Loss: 17.1083\n",
      "Epoch 4, Loss: 14.8922\n",
      "Epoch 4, Loss: 14.9319\n",
      "Epoch 4, Loss: 12.0415\n",
      "Epoch 4/200, Loss: 12.0415\n",
      "Epoch 5, Loss: 20.1865\n",
      "Epoch 5, Loss: 14.0098\n",
      "Epoch 5, Loss: 17.8445\n",
      "Epoch 5, Loss: 15.7061\n",
      "Epoch 5, Loss: 12.2882\n",
      "Epoch 5, Loss: 14.0094\n",
      "Epoch 5, Loss: 17.5266\n",
      "Epoch 5, Loss: 18.0301\n",
      "Epoch 5, Loss: 18.4101\n",
      "Epoch 5, Loss: 15.1352\n",
      "Epoch 5, Loss: 17.7555\n",
      "Epoch 5, Loss: 16.2969\n",
      "Epoch 5, Loss: 15.0659\n",
      "Epoch 5, Loss: 20.2458\n",
      "Epoch 5, Loss: 16.9371\n",
      "Epoch 5, Loss: 17.2760\n",
      "Epoch 5, Loss: 15.7006\n",
      "Epoch 5, Loss: 17.6791\n",
      "Epoch 5, Loss: 14.1033\n",
      "Epoch 5, Loss: 18.6763\n",
      "Epoch 5, Loss: 20.3179\n",
      "Epoch 5, Loss: 18.7760\n",
      "Epoch 5, Loss: 18.1062\n",
      "Epoch 5, Loss: 16.8491\n",
      "Epoch 5, Loss: 19.2933\n",
      "Epoch 5, Loss: 16.7158\n",
      "Epoch 5, Loss: 15.3218\n",
      "Epoch 5, Loss: 17.9070\n",
      "Epoch 5, Loss: 16.5018\n",
      "Epoch 5, Loss: 17.2729\n",
      "Epoch 5, Loss: 17.6310\n",
      "Epoch 5, Loss: 17.7431\n",
      "Epoch 5, Loss: 15.5596\n",
      "Epoch 5, Loss: 14.4924\n",
      "Epoch 5, Loss: 16.4624\n",
      "Epoch 5, Loss: 17.4304\n",
      "Epoch 5, Loss: 15.2764\n",
      "Epoch 5, Loss: 15.3089\n",
      "Epoch 5, Loss: 14.9094\n",
      "Epoch 5, Loss: 18.9131\n",
      "Epoch 5, Loss: 16.5858\n",
      "Epoch 5, Loss: 15.1591\n",
      "Epoch 5, Loss: 14.7507\n",
      "Epoch 5, Loss: 13.2079\n",
      "Epoch 5, Loss: 17.6258\n",
      "Epoch 5/200, Loss: 17.6258\n",
      "Epoch 6, Loss: 17.0723\n",
      "Epoch 6, Loss: 17.3805\n",
      "Epoch 6, Loss: 16.9215\n",
      "Epoch 6, Loss: 13.0832\n",
      "Epoch 6, Loss: 15.3571\n",
      "Epoch 6, Loss: 16.0872\n",
      "Epoch 6, Loss: 18.9051\n",
      "Epoch 6, Loss: 17.4110\n",
      "Epoch 6, Loss: 24.3605\n",
      "Epoch 6, Loss: 18.4300\n",
      "Epoch 6, Loss: 9.2835\n",
      "Epoch 6, Loss: 15.1547\n",
      "Epoch 6, Loss: 11.3566\n",
      "Epoch 6, Loss: 17.8750\n",
      "Epoch 6, Loss: 21.2701\n",
      "Epoch 6, Loss: 12.4773\n",
      "Epoch 6, Loss: 17.0086\n",
      "Epoch 6, Loss: 12.8198\n",
      "Epoch 6, Loss: 17.3306\n",
      "Epoch 6, Loss: 22.0447\n",
      "Epoch 6, Loss: 10.8190\n",
      "Epoch 6, Loss: 11.1624\n",
      "Epoch 6, Loss: 16.5910\n",
      "Epoch 6, Loss: 19.4127\n",
      "Epoch 6, Loss: 16.7017\n",
      "Epoch 6, Loss: 12.6827\n",
      "Epoch 6, Loss: 13.4363\n",
      "Epoch 6, Loss: 13.4147\n",
      "Epoch 6, Loss: 19.9126\n",
      "Epoch 6, Loss: 19.6135\n",
      "Epoch 6, Loss: 23.8399\n",
      "Epoch 6, Loss: 11.1541\n",
      "Epoch 6, Loss: 16.7621\n",
      "Epoch 6, Loss: 18.0809\n",
      "Epoch 6, Loss: 21.4142\n",
      "Epoch 6, Loss: 20.3307\n",
      "Epoch 6, Loss: 16.5862\n",
      "Epoch 6, Loss: 19.4042\n",
      "Epoch 6, Loss: 16.5389\n",
      "Epoch 6, Loss: 16.9624\n",
      "Epoch 6, Loss: 15.9631\n",
      "Epoch 6, Loss: 16.8777\n",
      "Epoch 6, Loss: 14.7584\n",
      "Epoch 6, Loss: 15.8140\n",
      "Epoch 6, Loss: 15.0140\n",
      "Epoch 6/200, Loss: 15.0140\n",
      "Epoch 7, Loss: 15.5702\n",
      "Epoch 7, Loss: 16.5486\n",
      "Epoch 7, Loss: 15.8694\n",
      "Epoch 7, Loss: 14.2729\n",
      "Epoch 7, Loss: 18.9058\n",
      "Epoch 7, Loss: 15.6842\n",
      "Epoch 7, Loss: 16.7566\n",
      "Epoch 7, Loss: 18.4012\n",
      "Epoch 7, Loss: 17.9096\n",
      "Epoch 7, Loss: 17.7092\n",
      "Epoch 7, Loss: 18.1546\n",
      "Epoch 7, Loss: 15.8023\n",
      "Epoch 7, Loss: 14.2984\n",
      "Epoch 7, Loss: 16.9598\n",
      "Epoch 7, Loss: 14.5365\n",
      "Epoch 7, Loss: 19.0937\n",
      "Epoch 7, Loss: 18.0237\n",
      "Epoch 7, Loss: 16.0518\n",
      "Epoch 7, Loss: 16.2678\n",
      "Epoch 7, Loss: 14.3123\n",
      "Epoch 7, Loss: 18.3417\n",
      "Epoch 7, Loss: 14.6207\n",
      "Epoch 7, Loss: 17.1291\n",
      "Epoch 7, Loss: 16.2496\n",
      "Epoch 7, Loss: 17.3065\n",
      "Epoch 7, Loss: 15.7597\n",
      "Epoch 7, Loss: 14.8546\n",
      "Epoch 7, Loss: 16.8803\n",
      "Epoch 7, Loss: 15.7285\n",
      "Epoch 7, Loss: 18.5292\n",
      "Epoch 7, Loss: 15.9987\n",
      "Epoch 7, Loss: 15.7570\n",
      "Epoch 7, Loss: 17.8506\n",
      "Epoch 7, Loss: 17.2581\n",
      "Epoch 7, Loss: 14.9430\n",
      "Epoch 7, Loss: 15.3308\n",
      "Epoch 7, Loss: 16.7824\n",
      "Epoch 7, Loss: 14.6457\n",
      "Epoch 7, Loss: 18.7270\n",
      "Epoch 7, Loss: 16.6338\n",
      "Epoch 7, Loss: 16.1843\n",
      "Epoch 7, Loss: 16.7051\n",
      "Epoch 7, Loss: 16.8417\n",
      "Epoch 7, Loss: 17.4654\n",
      "Epoch 7, Loss: 16.3059\n",
      "Epoch 7/200, Loss: 16.3059\n",
      "Epoch 8, Loss: 16.0001\n",
      "Epoch 8, Loss: 15.1597\n",
      "Epoch 8, Loss: 16.3053\n",
      "Epoch 8, Loss: 16.0848\n",
      "Epoch 8, Loss: 15.7869\n",
      "Epoch 8, Loss: 15.6123\n",
      "Epoch 8, Loss: 14.5076\n",
      "Epoch 8, Loss: 17.9845\n",
      "Epoch 8, Loss: 15.9058\n",
      "Epoch 8, Loss: 12.9803\n",
      "Epoch 8, Loss: 18.7610\n",
      "Epoch 8, Loss: 13.4762\n",
      "Epoch 8, Loss: 13.2976\n",
      "Epoch 8, Loss: 15.8996\n",
      "Epoch 8, Loss: 21.0402\n",
      "Epoch 8, Loss: 19.5199\n",
      "Epoch 8, Loss: 15.6135\n",
      "Epoch 8, Loss: 17.2783\n",
      "Epoch 8, Loss: 16.1402\n",
      "Epoch 8, Loss: 16.5716\n",
      "Epoch 8, Loss: 18.1621\n",
      "Epoch 8, Loss: 17.5716\n",
      "Epoch 8, Loss: 13.9408\n",
      "Epoch 8, Loss: 17.4942\n",
      "Epoch 8, Loss: 11.2220\n",
      "Epoch 8, Loss: 14.8952\n",
      "Epoch 8, Loss: 21.2520\n",
      "Epoch 8, Loss: 19.8408\n",
      "Epoch 8, Loss: 12.8151\n",
      "Epoch 8, Loss: 14.1588\n",
      "Epoch 8, Loss: 21.8167\n",
      "Epoch 8, Loss: 12.4377\n",
      "Epoch 8, Loss: 17.8793\n",
      "Epoch 8, Loss: 15.1006\n",
      "Epoch 8, Loss: 13.5222\n",
      "Epoch 8, Loss: 18.0165\n",
      "Epoch 8, Loss: 13.8850\n",
      "Epoch 8, Loss: 16.4345\n",
      "Epoch 8, Loss: 23.5613\n",
      "Epoch 8, Loss: 18.2969\n",
      "Epoch 8, Loss: 14.3328\n",
      "Epoch 8, Loss: 17.8582\n",
      "Epoch 8, Loss: 14.3106\n",
      "Epoch 8, Loss: 12.7073\n",
      "Epoch 8, Loss: 15.2241\n",
      "Epoch 8/200, Loss: 15.2241\n",
      "Epoch 9, Loss: 16.8762\n",
      "Epoch 9, Loss: 15.5692\n",
      "Epoch 9, Loss: 18.6515\n",
      "Epoch 9, Loss: 17.9293\n",
      "Epoch 9, Loss: 20.8537\n",
      "Epoch 9, Loss: 16.0230\n",
      "Epoch 9, Loss: 18.1505\n",
      "Epoch 9, Loss: 15.2031\n",
      "Epoch 9, Loss: 17.9932\n",
      "Epoch 9, Loss: 17.0355\n",
      "Epoch 9, Loss: 14.5623\n",
      "Epoch 9, Loss: 15.8449\n",
      "Epoch 9, Loss: 18.6083\n",
      "Epoch 9, Loss: 17.0714\n",
      "Epoch 9, Loss: 16.3721\n",
      "Epoch 9, Loss: 17.1793\n",
      "Epoch 9, Loss: 16.8696\n",
      "Epoch 9, Loss: 16.7800\n",
      "Epoch 9, Loss: 15.1296\n",
      "Epoch 9, Loss: 16.6694\n",
      "Epoch 9, Loss: 17.9284\n",
      "Epoch 9, Loss: 18.6375\n",
      "Epoch 9, Loss: 15.6137\n",
      "Epoch 9, Loss: 16.9029\n",
      "Epoch 9, Loss: 15.5089\n",
      "Epoch 9, Loss: 16.7862\n",
      "Epoch 9, Loss: 15.7914\n",
      "Epoch 9, Loss: 16.1372\n",
      "Epoch 9, Loss: 17.5086\n",
      "Epoch 9, Loss: 16.0206\n",
      "Epoch 9, Loss: 16.3979\n",
      "Epoch 9, Loss: 16.0335\n",
      "Epoch 9, Loss: 16.8705\n",
      "Epoch 9, Loss: 14.0912\n",
      "Epoch 9, Loss: 18.5879\n",
      "Epoch 9, Loss: 15.9494\n",
      "Epoch 9, Loss: 14.8871\n",
      "Epoch 9, Loss: 19.3437\n",
      "Epoch 9, Loss: 18.1177\n",
      "Epoch 9, Loss: 17.1617\n",
      "Epoch 9, Loss: 16.4966\n",
      "Epoch 9, Loss: 16.2468\n",
      "Epoch 9, Loss: 16.1519\n",
      "Epoch 9, Loss: 17.2344\n",
      "Epoch 9, Loss: 19.2893\n",
      "Epoch 9/200, Loss: 19.2893\n",
      "Epoch 10, Loss: 16.7688\n",
      "Epoch 10, Loss: 16.7750\n",
      "Epoch 10, Loss: 16.2907\n",
      "Epoch 10, Loss: 16.5813\n",
      "Epoch 10, Loss: 15.8968\n",
      "Epoch 10, Loss: 15.8263\n",
      "Epoch 10, Loss: 17.6391\n",
      "Epoch 10, Loss: 15.8193\n",
      "Epoch 10, Loss: 16.2681\n",
      "Epoch 10, Loss: 16.2479\n",
      "Epoch 10, Loss: 15.8228\n",
      "Epoch 10, Loss: 16.5943\n",
      "Epoch 10, Loss: 16.2827\n",
      "Epoch 10, Loss: 18.5194\n",
      "Epoch 10, Loss: 16.3774\n",
      "Epoch 10, Loss: 16.4413\n",
      "Epoch 10, Loss: 18.1265\n",
      "Epoch 10, Loss: 14.1828\n",
      "Epoch 10, Loss: 15.6574\n",
      "Epoch 10, Loss: 17.6838\n",
      "Epoch 10, Loss: 16.4752\n",
      "Epoch 10, Loss: 16.0927\n",
      "Epoch 10, Loss: 15.4177\n",
      "Epoch 10, Loss: 15.8778\n",
      "Epoch 10, Loss: 15.9915\n",
      "Epoch 10, Loss: 19.2378\n",
      "Epoch 10, Loss: 15.4526\n",
      "Epoch 10, Loss: 16.5509\n",
      "Epoch 10, Loss: 19.0497\n",
      "Epoch 10, Loss: 14.4728\n",
      "Epoch 10, Loss: 16.4050\n",
      "Epoch 10, Loss: 14.9305\n",
      "Epoch 10, Loss: 13.2387\n",
      "Epoch 10, Loss: 17.6979\n",
      "Epoch 10, Loss: 15.0485\n",
      "Epoch 10, Loss: 14.9209\n",
      "Epoch 10, Loss: 16.4036\n",
      "Epoch 10, Loss: 17.7827\n",
      "Epoch 10, Loss: 19.1912\n",
      "Epoch 10, Loss: 15.7525\n",
      "Epoch 10, Loss: 19.9187\n",
      "Epoch 10, Loss: 20.0929\n",
      "Epoch 10, Loss: 15.3575\n",
      "Epoch 10, Loss: 14.8627\n",
      "Epoch 10, Loss: 23.4185\n",
      "Epoch 10/200, Loss: 23.4185\n"
     ]
    }
   ],
   "source": [
    "# Assuming model, optimizers, and loss functions are already defined\n",
    "model = SiameseNetwork().to(device)\n",
    "optimizer_contrastive = optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_arcface = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "criterion_contrastive = ContrastiveLoss(margin=2.0)\n",
    "criterion_arcface = ArcFaceLoss(s=30, m=0.5)\n",
    "\n",
    "# Scheduler for learning rate decay of the ArcFace optimizer\n",
    "scheduler_arcface = optim.lr_scheduler.LinearLR(optimizer_arcface, start_factor=1.0, end_factor=0, total_iters=200)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Total epochs set as 200\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for img1, img2, labels in paired_train_dataloader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "        output1, output2 = model(img1, img2)\n",
    "        \n",
    "        # Compute Contrastive Loss\n",
    "        loss_contrastive = criterion_contrastive(output1, output2, labels)\n",
    "        \n",
    "        # Calculate the mean embedding for ArcFace Loss (simplified version)\n",
    "        embeddings = (output1 + output2) / 2\n",
    "        loss_arcface = criterion_arcface(embeddings, labels)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = loss_contrastive + loss_arcface\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss.item():.4f}')\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer_contrastive.zero_grad()\n",
    "        optimizer_arcface.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer_contrastive.step()\n",
    "        optimizer_arcface.step()\n",
    "    \n",
    "    scheduler_arcface.step()  # Update the learning rate\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/200, Loss: {total_loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = SiameseNetwork().cuda()\n",
    "model.load_state_dict(torch.load('path_to_saved_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "# Change the device to CPU\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_similarity(img_path1, img_path2):\n",
    "    # Load and transform images\n",
    "    img1 = Image.open(img_path1).convert('RGB')\n",
    "    img2 = Image.open(img_path2).convert('RGB')\n",
    "    img1 = transform(img1).unsqueeze(0).to(device) # Add batch dimension and send to GPU\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)  # Add batch dimension and send to GPU\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        output1, output2 = model(img1, img2)\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "\n",
    "    return euclidean_distance.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between the images is: 1.481567621231079\n"
     ]
    }
   ],
   "source": [
    "# Paths to the images you want to compare\n",
    "image_path1 = 'dataset/train/1771/A*d-fSRoB7LOAAAAAAAAAAAAAAAQAAAQ.jpg'\n",
    "image_path2 = 'dataset/train/1775/A*8dSMQ6vUnmUAAAAAAAAAAAAAAQAAAQ.jpg'\n",
    "\n",
    "# Perform inference\n",
    "distance = infer_similarity(image_path1, image_path2)\n",
    "print(f'The Euclidean distance between the images is: {distance}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar image: dataset/train/1790/A*Kf4tQrPJVrgAAAAAAAAAAAAAAQAAAQ.jpg, Distance: 2.301255226135254\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(query_img_path, reference_img_paths):\n",
    "    min_distance = float('inf')\n",
    "    most_similar_img = None\n",
    "\n",
    "    for ref_path in reference_img_paths:\n",
    "        distance = infer_similarity(query_img_path, ref_path)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            most_similar_img = ref_path\n",
    "\n",
    "    return most_similar_img, min_distance\n",
    "\n",
    "# List of reference image paths\n",
    "reference_images = ['dataset/train/1781/0Tolu7cpRQurgl7b87DAOwAAACMAARAD.jpg','dataset/train/1790/A*Kf4tQrPJVrgAAAAAAAAAAAAAAQAAAQ.jpg', 'dataset/train/1771/A*jP8SQ4cdHVDm2wJBthDn0AAAAQAAAQ.jpg', 'dataset/train/1775/A*cSJSR4QTq74AAAAAAAAAAAAAAQAAAQ.jpg', 'dataset/train/1780/A*83ofSqE-8DMAAAAAAAAAAAAAAQAAAQ.jpg']\n",
    "most_similar_image, similarity_score = find_most_similar('dataset/train/1790/A*o2r2TqdLuOoAAAAAAAAAAAAAAQAAAQ.jpg', reference_images)\n",
    "print(f'Most similar image: {most_similar_image}, Distance: {similarity_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog_nose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
